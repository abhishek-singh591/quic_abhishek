/home/abhishek/eff/lib/python3.10/site-packages/onnxscript/converter.py:816: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.
  param_schemas = callee.param_schemas()
Some weights of the model checkpoint at meta-llama/Llama-3.2-1B were not used when initializing LlamaForCausalLM: ['model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m[Warning]: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs![0m
[93m[Warning]: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs![0m
[93m[Warning]: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.[0m
[93m[Warning]: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.[0m
[93m[Warning]: ONNX Preprocess - Removing mutation from node aten::index_put_ on block input: '2'. This changes graph semantics. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:348.)[0m
[93m[Warning]: ONNX Preprocess - Removing mutation from node aten::index_put_ on block input: '3'. This changes graph semantics. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:348.)[0m
[93m[Warning]: ONNX Preprocess - Removing mutation from node aten::index_put_ on block input: '4'. This changes graph semantics. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:348.)[0m
[93m[Warning]: ONNX Preprocess - Removing mutation from node aten::index_put_ on block input: '5'. This changes graph semantics. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:348.)[0m
[93m[Warning]: ONNX Preprocess - Removing mutation from node aten::index_put_ on block input: '6'. This changes graph semantics. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:348.)[0m
[93m[Warning]: ONNX Preprocess - Removing mutation from node aten::index_put_ on block input: '7'. This changes graph semantics. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:348.)[0m
[93m[Warning]: ONNX Preprocess - Removing mutation from node aten::index_put_ on block input: '8'. This changes graph semantics. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:348.)[0m
[93m[Warning]: ONNX Preprocess - Removing mutation from node aten::index_put_ on block input: '9'. This changes graph semantics. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:348.)[0m
Torch IR graph at exception: graph(%input.1 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu),
      %position_ids.1 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu),
      %2 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %3 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %4 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %5 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %6 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %7 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %8 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %9 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %model.embed_tokens.weight : Float(128256, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.0.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.0.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.1.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.1.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.2.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.2.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.3.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.3.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.norm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu)):
  %16837 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/torch.nn.modules.sparse.Embedding::embed_tokens
  %16838 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/torch.nn.modules.sparse.Embedding::embed_tokens
  %hidden_states.1 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::embedding(%model.embed_tokens.weight, %input.1, %16837, %16838, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/torch.nn.modules.sparse.Embedding::embed_tokens # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2551:0
  %16839 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %12262 : Long(device=cpu) = aten::size(%2, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %12284 : Long(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.1, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:43:0
  %16840 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %12286 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %12288 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:44:0
  %12290 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16840, %12262, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:44:0
  %16644 : int[] = prim::Constant[value=[1, 1, -1]]()
  %12295 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::view(%12290, %16644), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:44:0
  %12296 : Bool(1, 32, 32, strides=[1024, 32, 1], requires_grad=0, device=cpu) = aten::gt(%12295, %12284), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:45:0
  %16841 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %attention_mask : Bool(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12296, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:46:0
  %12299 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.1, %model.layers.0.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.73 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15298 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16842 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15300 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.73, %16842), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16645 : int[] = prim::Constant[value=[-1]]()
      %16843 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15304 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15305 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15300, %16645, %16843, %15304), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16844 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16845 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16748 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15305, %16845, %16844), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15309 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16748), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15310 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.73, %15309), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15311 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15298, %15310), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15311)
  %12314 : Long(device=cpu) = aten::size(%12299, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %12321 : Long(device=cpu) = aten::size(%12299, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %12331 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%12299, %model.layers.0.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %16846 : Long(device=cpu) = prim::Constant[value={64}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12334 : int[] = prim::ListConstruct(%12314, %12321, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12335 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%12331, %12334), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %16847 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %query_states.1 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%12335, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %12340 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%12299, %model.layers.0.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %12344 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%12340, %12334), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.1 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%12344, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %12349 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%12299, %model.layers.0.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %12353 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%12349, %12334), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.1 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%12353, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %12371 : Long(device=cpu) = aten::size(%2, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %12393 : Float(131072, 64, strides=[64, 1], requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %12397 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %12371, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16848 : Long(device=cpu) = prim::Constant[value={6}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb
  %12402 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%12397, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16849 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos.1 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%12402, %16849), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %12405 : Float(131072, 64, strides=[64, 1], requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %12409 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %12371, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %12414 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%12409, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16850 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin.1 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%12414, %16850), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %12420 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12421 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos.1, %12420), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %12423 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12421, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %12424 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12425 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin.1, %12424), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %12427 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12425, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %12428 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.1, %12423), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %12439 : Long(device=cpu) = aten::size(%query_states.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %12441 : Long(requires_grad=0, device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %12442 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%12439, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %12447 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.1, %16839, %16840, %12442, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %16851 : Long(device=cpu) = prim::Constant[value={9223372036854775807}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12466 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.1, %16839, %12442, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %12467 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%12466), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12468 : Tensor[] = prim::ListConstruct(%12467, %12447), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12470 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%12468, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12471 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%12470, %12427), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %12473 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%12428, %12471, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %12474 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.1, %12423), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %12485 : Long(device=cpu) = aten::size(%key_states.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %12488 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%12485, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %12493 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.1, %16839, %16840, %12488, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %12512 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.1, %16839, %12488, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %12513 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%12512), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12514 : Tensor[] = prim::ListConstruct(%12513, %12493), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12516 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%12514, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12517 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%12516, %12427), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %12519 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%12474, %12517, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states.3 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%12473, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.3 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%12519, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.1 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%2, %position_ids.1, %key_states.3), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15312 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.3 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states.81 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16852 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15316 : Long(device=cpu) = aten::size(%15312, %16852), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %15328 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15329 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15330 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16853 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15332 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15316, %15328, %15329, %15330, %16853), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16646 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15338 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15332, %16646), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16854 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15343 : Long(device=cpu) = aten::size(%15312, %16854), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %15352 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15353 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15354 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16855 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15356 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15343, %15352, %15353, %15354, %16855), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16647 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15362 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15356, %16647), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16856 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15370 : Long(device=cpu) = aten::size(%15312, %16856), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %15376 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15377 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15378 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16857 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15380 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15370, %15376, %15377, %15378, %16857), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16648 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15386 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15380, %16648), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16858 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15388 : Long(device=cpu) = aten::size(%15312, %16858), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16859 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16860 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16861 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15403 : int[] = prim::ListConstruct(%15388, %16859, %16860, %16861), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15404 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.3, %15403), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16862 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16863 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15407 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states.81, %16862, %16863), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16864 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15409 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%15407, %16864), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16649 : int[] = prim::Constant[value=[8, 64, 32]]()
      %15414 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%15409, %16649), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15415 : Tensor?[] = prim::ListConstruct(%15338, %15362, %15386, %15404), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16865 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15417 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%15312, %15415, %15414, %16865), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15418 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%15417, %15417), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15418)
  %v_out.1 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%3, %position_ids.1, %value_states.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15419 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids.5 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states.41 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16866 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15423 : Long(device=cpu) = aten::size(%15419, %16866), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %15435 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15436 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15437 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16867 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15439 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15423, %15435, %15436, %15437, %16867), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16654 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15444 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15439, %16654), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16868 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15449 : Long(device=cpu) = aten::size(%15419, %16868), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %15458 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15459 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15460 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16869 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15462 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15449, %15458, %15459, %15460, %16869), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16655 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15467 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15462, %16655), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16870 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15469 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.5, %16870), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16656 : int[] = prim::Constant[value=[8, 32, 64]]()
      %15474 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states.41, %16656), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15475 : Tensor?[] = prim::ListConstruct(%15444, %15467, %15469), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16871 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15477 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%15419, %15475, %15474, %16871), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15478 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%15477, %15477), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15478)
  %12703 : Long(device=cpu) = aten::size(%k_out.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %12710 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%12703, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %12712 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12710, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %12714 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12712, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %16872 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12717 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %12718 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %12720 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12717, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %12721 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%12714, %12720), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %16873 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %ctx_indices.1 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%12721, %16873, %12714), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key.1 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.1, %ctx_indices.1, %12703), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out.41 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.41 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.41 : Long(device=cpu)):
      %16874 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15483 : Long(device=cpu) = aten::size(%k_out.41, %16874), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %15495 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15496 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15497 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16875 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15499 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15483, %15495, %15496, %15497, %16875), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16660 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15505 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15499, %16660), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16876 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15510 : Long(device=cpu) = aten::size(%k_out.41, %16876), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %15519 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15520 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15521 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16877 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15523 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15510, %15519, %15520, %15521, %16877), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16661 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15529 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15523, %16661), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16878 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15537 : Long(device=cpu) = aten::size(%k_out.41, %16878), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %15543 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15544 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15545 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16879 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15547 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15537, %15543, %15544, %15545, %16879), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16662 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15553 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15547, %16662), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16880 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15555 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.41, %16880), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16881 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15557 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15555, %16881, %ctx_indices.41), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %15558 : Tensor?[] = prim::ListConstruct(%15505, %15529, %15553, %15557), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15559 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out.41, %15558), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%15559)
  %12803 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.1, %ctx_indices.1, %12703), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out.41 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices.43 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.43 : Long(device=cpu)):
      %16882 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15564 : Long(device=cpu) = aten::size(%v_out.41, %16882), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %15576 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15577 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15578 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16883 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15580 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15564, %15576, %15577, %15578, %16883), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16663 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15585 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15580, %16663), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16884 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15590 : Long(device=cpu) = aten::size(%v_out.41, %16884), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %15599 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15600 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15601 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16885 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15603 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15590, %15599, %15600, %15601, %16885), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16664 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15608 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15603, %16664), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16886 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15610 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.43, %16886), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16887 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15612 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15610, %16887, %ctx_indices.43), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %15613 : Tensor?[] = prim::ListConstruct(%15585, %15608, %15612), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15614 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out.41, %15613), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%15614)
  %12857 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12721, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %16666 : Float(requires_grad=0, device=cpu) = prim::Constant[value={0}]()
  %value.1 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%12857, %16666, %12803), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %12868 : Long(device=cpu) = aten::size(%key.1, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12873 : Long(device=cpu) = aten::size(%key.1, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12877 : Long(device=cpu) = aten::size(%key.1, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12882 : Long(device=cpu) = aten::size(%key.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12890 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key.1, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12895 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%12890, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12897 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%12895, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12902 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%12897, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16888 : Long(device=cpu) = prim::Constant[value={4}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12907 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%12902, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12909 : int[] = prim::ListConstruct(%12868, %12873, %16888, %12877, %12882), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12911 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%12907, %12909, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16889 : Long(device=cpu) = prim::Constant[value={4}]()
  %16754 : Long(requires_grad=0, device=cpu) = aten::mul(%12873, %16889), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12915 : int[] = prim::ListConstruct(%12868, %16754, %12877, %12882), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12916 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%12911, %12915), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12918 : Long(device=cpu) = aten::size(%value.1, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12923 : Long(device=cpu) = aten::size(%value.1, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12927 : Long(device=cpu) = aten::size(%value.1, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12932 : Long(device=cpu) = aten::size(%value.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12940 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value.1, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12945 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%12940, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12947 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%12945, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12952 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%12947, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12957 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%12952, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12959 : int[] = prim::ListConstruct(%12918, %12923, %16888, %12927, %12932), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12961 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%12957, %12959, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16890 : Long(device=cpu) = prim::Constant[value={4}]()
  %16756 : Long(requires_grad=0, device=cpu) = aten::mul(%12923, %16890), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12965 : int[] = prim::ListConstruct(%12918, %16756, %12927, %12932), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12966 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%12961, %12965), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12967 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states.3, %12916), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16891 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16758 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%12967, %16891), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16668 : Float(requires_grad=0, device=cpu) = prim::Constant[value={-inf}]()
  %attn_weights.1 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16758), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %12981 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights.1, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %12986 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%12981, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %12987 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%12986, %12966), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %12990 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%12987, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %12992 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%12990, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %12994 : int[] = prim::ListConstruct(%12314, %12321, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12995 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%12992, %12994), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %12997 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%12995, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %12999 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%12997, %model.layers.0.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.3 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.1, %12999, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %13006 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.3, %model.layers.0.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.75 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15616 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16892 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15618 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.75, %16892), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16669 : int[] = prim::Constant[value=[-1]]()
      %16893 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15622 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15623 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15618, %16669, %16893, %15622), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16894 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16895 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16760 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15623, %16895, %16894), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15627 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16760), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15628 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.75, %15627), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15629 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15616, %15628), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15629)
  %input.3 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13006, %model.layers.0.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13022 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input.3), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %13024 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13006, %model.layers.0.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13025 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%13022, %13024), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %13027 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13025, %model.layers.0.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.5 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.3, %13027, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %13034 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.5, %model.layers.1.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.77 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15631 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16896 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15633 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.77, %16896), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16670 : int[] = prim::Constant[value=[-1]]()
      %16897 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15637 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15638 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15633, %16670, %16897, %15637), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16898 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16899 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16762 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15638, %16899, %16898), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15642 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16762), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15643 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.77, %15642), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15644 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15631, %15643), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15644)
  %13049 : Long(device=cpu) = aten::size(%13034, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13056 : Long(device=cpu) = aten::size(%13034, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13066 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13034, %model.layers.1.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13069 : int[] = prim::ListConstruct(%13049, %13056, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13070 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%13066, %13069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %query_states.5 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%13070, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %13075 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13034, %model.layers.1.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13079 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13075, %13069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.5 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13079, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %13084 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13034, %model.layers.1.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13088 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13084, %13069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.3 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13088, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %13106 : Long(device=cpu) = aten::size(%4, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %13132 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %13106, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13137 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13132, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16900 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos.3 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13137, %16900), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13144 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %13106, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13149 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13144, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16901 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin.3 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13149, %16901), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13155 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13156 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos.3, %13155), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13158 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13156, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13159 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13160 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin.3, %13159), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13162 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13160, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13163 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.5, %13158), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13174 : Long(device=cpu) = aten::size(%query_states.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13177 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13174, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13182 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.5, %16839, %16840, %13177, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13201 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.5, %16839, %13177, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13202 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%13201), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13203 : Tensor[] = prim::ListConstruct(%13202, %13182), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13205 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13203, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13206 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13205, %13162), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13208 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%13163, %13206, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13209 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.5, %13158), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13220 : Long(device=cpu) = aten::size(%key_states.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13223 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13220, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13228 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.5, %16839, %16840, %13223, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13247 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.5, %16839, %13223, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13248 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%13247), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13249 : Tensor[] = prim::ListConstruct(%13248, %13228), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13251 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13249, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13252 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13251, %13162), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13254 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%13209, %13252, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states.7 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%13208, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.7 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%13254, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.3 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%4, %position_ids.1, %key_states.7), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15645 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.7 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states.83 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16902 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15649 : Long(device=cpu) = aten::size(%15645, %16902), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %15661 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15662 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15663 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16903 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15665 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15649, %15661, %15662, %15663, %16903), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16671 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15671 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15665, %16671), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16904 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15676 : Long(device=cpu) = aten::size(%15645, %16904), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %15685 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15686 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15687 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16905 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15689 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15676, %15685, %15686, %15687, %16905), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16672 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15695 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15689, %16672), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16906 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15703 : Long(device=cpu) = aten::size(%15645, %16906), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %15709 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15710 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15711 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16907 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15713 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15703, %15709, %15710, %15711, %16907), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16673 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15719 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15713, %16673), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16908 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15721 : Long(device=cpu) = aten::size(%15645, %16908), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16909 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16910 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16911 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15736 : int[] = prim::ListConstruct(%15721, %16909, %16910, %16911), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15737 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.7, %15736), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16912 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16913 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15740 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states.83, %16912, %16913), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16914 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15742 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%15740, %16914), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16674 : int[] = prim::Constant[value=[8, 64, 32]]()
      %15747 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%15742, %16674), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15748 : Tensor?[] = prim::ListConstruct(%15671, %15695, %15719, %15737), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16915 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15750 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%15645, %15748, %15747, %16915), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15751 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%15750, %15750), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15751)
  %v_out.3 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%5, %position_ids.1, %value_states.3), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15752 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids.9 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states.43 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16916 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15756 : Long(device=cpu) = aten::size(%15752, %16916), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %15768 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15769 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15770 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16917 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15772 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15756, %15768, %15769, %15770, %16917), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16679 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15777 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15772, %16679), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16918 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15782 : Long(device=cpu) = aten::size(%15752, %16918), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %15791 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15792 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15793 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16919 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15795 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15782, %15791, %15792, %15793, %16919), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16680 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15800 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15795, %16680), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16920 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15802 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.9, %16920), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16681 : int[] = prim::Constant[value=[8, 32, 64]]()
      %15807 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states.43, %16681), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15808 : Tensor?[] = prim::ListConstruct(%15777, %15800, %15802), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16921 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15810 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%15752, %15808, %15807, %16921), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15811 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%15810, %15810), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15811)
  %13438 : Long(device=cpu) = aten::size(%k_out.3, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %13445 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%13438, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %13447 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13445, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %13449 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13447, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %13452 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %13453 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %13455 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13452, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %13456 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%13449, %13455), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %ctx_indices.3 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%13456, %16873, %13449), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key.3 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.3, %ctx_indices.3, %13438), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out.43 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.45 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.45 : Long(device=cpu)):
      %16922 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15816 : Long(device=cpu) = aten::size(%k_out.43, %16922), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %15828 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15829 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15830 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16923 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15832 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15816, %15828, %15829, %15830, %16923), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16685 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15838 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15832, %16685), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16924 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15843 : Long(device=cpu) = aten::size(%k_out.43, %16924), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %15852 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15853 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15854 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16925 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15856 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15843, %15852, %15853, %15854, %16925), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16686 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15862 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15856, %16686), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16926 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15870 : Long(device=cpu) = aten::size(%k_out.43, %16926), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %15876 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15877 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15878 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16927 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15880 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15870, %15876, %15877, %15878, %16927), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16687 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15886 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15880, %16687), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16928 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15888 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.45, %16928), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16929 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15890 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15888, %16929, %ctx_indices.45), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %15891 : Tensor?[] = prim::ListConstruct(%15838, %15862, %15886, %15890), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15892 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out.43, %15891), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%15892)
  %13538 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.3, %ctx_indices.3, %13438), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out.43 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices.47 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.47 : Long(device=cpu)):
      %16930 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15897 : Long(device=cpu) = aten::size(%v_out.43, %16930), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %15909 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15910 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15911 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16931 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15913 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15897, %15909, %15910, %15911, %16931), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16688 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15918 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15913, %16688), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16932 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15923 : Long(device=cpu) = aten::size(%v_out.43, %16932), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %15932 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15933 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15934 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16933 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15936 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15923, %15932, %15933, %15934, %16933), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16689 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15941 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15936, %16689), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16934 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15943 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.47, %16934), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16935 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15945 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15943, %16935, %ctx_indices.47), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %15946 : Tensor?[] = prim::ListConstruct(%15918, %15941, %15945), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15947 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out.43, %15946), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%15947)
  %13592 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13456, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %value.3 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%13592, %16666, %13538), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %13603 : Long(device=cpu) = aten::size(%key.3, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13608 : Long(device=cpu) = aten::size(%key.3, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13612 : Long(device=cpu) = aten::size(%key.3, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13617 : Long(device=cpu) = aten::size(%key.3, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13625 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key.3, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13630 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%13625, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13632 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%13630, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13637 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%13632, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13642 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%13637, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13644 : int[] = prim::ListConstruct(%13603, %13608, %16888, %13612, %13617), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13646 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%13642, %13644, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16936 : Long(device=cpu) = prim::Constant[value={4}]()
  %16768 : Long(requires_grad=0, device=cpu) = aten::mul(%13608, %16936), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13650 : int[] = prim::ListConstruct(%13603, %16768, %13612, %13617), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13651 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%13646, %13650), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13653 : Long(device=cpu) = aten::size(%value.3, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13658 : Long(device=cpu) = aten::size(%value.3, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13662 : Long(device=cpu) = aten::size(%value.3, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13667 : Long(device=cpu) = aten::size(%value.3, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13675 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value.3, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13680 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%13675, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13682 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%13680, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13687 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%13682, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13692 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%13687, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13694 : int[] = prim::ListConstruct(%13653, %13658, %16888, %13662, %13667), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13696 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%13692, %13694, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16937 : Long(device=cpu) = prim::Constant[value={4}]()
  %16770 : Long(requires_grad=0, device=cpu) = aten::mul(%13658, %16937), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13700 : int[] = prim::ListConstruct(%13653, %16770, %13662, %13667), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13701 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%13696, %13700), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13702 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states.7, %13651), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16938 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16772 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%13702, %16938), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %attn_weights.3 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16772), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %13716 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights.3, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %13721 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%13716, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %13722 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%13721, %13701), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %13725 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%13722, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %13727 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%13725, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %13729 : int[] = prim::ListConstruct(%13049, %13056, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13730 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%13727, %13729), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %13732 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%13730, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %13734 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13732, %model.layers.1.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.7 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.5, %13734, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %13741 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.7, %model.layers.1.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.79 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15949 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16939 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15951 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.79, %16939), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16694 : int[] = prim::Constant[value=[-1]]()
      %16940 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15955 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15956 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15951, %16694, %16940, %15955), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16941 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16942 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16774 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15956, %16942, %16941), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15960 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16774), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15961 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.79, %15960), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15962 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15949, %15961), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15962)
  %input.5 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13741, %model.layers.1.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13757 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input.5), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %13759 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13741, %model.layers.1.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13760 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%13757, %13759), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %13762 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13760, %model.layers.1.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.9 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.7, %13762, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %13769 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.9, %model.layers.2.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.81 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15964 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16943 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15966 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.81, %16943), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16695 : int[] = prim::Constant[value=[-1]]()
      %16944 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15970 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15971 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15966, %16695, %16944, %15970), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16945 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16946 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16776 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15971, %16946, %16945), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15975 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16776), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15976 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.81, %15975), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15977 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15964, %15976), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15977)
  %13784 : Long(device=cpu) = aten::size(%13769, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13791 : Long(device=cpu) = aten::size(%13769, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13801 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13769, %model.layers.2.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13804 : int[] = prim::ListConstruct(%13784, %13791, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13805 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%13801, %13804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %query_states.9 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%13805, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %13810 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13769, %model.layers.2.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13814 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13810, %13804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.9 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13814, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %13819 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13769, %model.layers.2.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13823 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13819, %13804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.5 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13823, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %13841 : Long(device=cpu) = aten::size(%6, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %13867 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %13841, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13872 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13867, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16947 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos.5 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13872, %16947), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13879 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %13841, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13884 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13879, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16948 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin.5 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13884, %16948), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13890 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13891 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos.5, %13890), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13893 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13891, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13894 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13895 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin.5, %13894), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13897 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13895, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13898 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.9, %13893), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13909 : Long(device=cpu) = aten::size(%query_states.9, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13912 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13909, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13917 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.9, %16839, %16840, %13912, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13936 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.9, %16839, %13912, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13937 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%13936), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13938 : Tensor[] = prim::ListConstruct(%13937, %13917), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13940 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13938, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13941 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13940, %13897), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13943 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%13898, %13941, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13944 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.9, %13893), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13955 : Long(device=cpu) = aten::size(%key_states.9, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13958 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13955, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13963 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.9, %16839, %16840, %13958, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13982 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.9, %16839, %13958, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13983 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%13982), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13984 : Tensor[] = prim::ListConstruct(%13983, %13963), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13986 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13984, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13987 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13986, %13897), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13989 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%13944, %13987, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states.11 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%13943, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.11 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%13989, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.5 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%6, %position_ids.1, %key_states.11), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15978 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.11 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states.85 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16949 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15982 : Long(device=cpu) = aten::size(%15978, %16949), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %15994 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15995 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15996 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16950 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15998 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15982, %15994, %15995, %15996, %16950), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16696 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16004 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15998, %16696), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16951 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16009 : Long(device=cpu) = aten::size(%15978, %16951), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16018 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16019 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16020 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16952 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16022 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16009, %16018, %16019, %16020, %16952), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16697 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16028 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16022, %16697), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16953 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16036 : Long(device=cpu) = aten::size(%15978, %16953), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16042 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16043 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16044 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16954 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16046 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16036, %16042, %16043, %16044, %16954), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16698 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16052 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16046, %16698), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16955 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16054 : Long(device=cpu) = aten::size(%15978, %16955), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16956 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16957 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16958 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16069 : int[] = prim::ListConstruct(%16054, %16956, %16957, %16958), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16070 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.11, %16069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16959 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16960 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16073 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states.85, %16959, %16960), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16961 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16075 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%16073, %16961), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16699 : int[] = prim::Constant[value=[8, 64, 32]]()
      %16080 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%16075, %16699), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16081 : Tensor?[] = prim::ListConstruct(%16004, %16028, %16052, %16070), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16962 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16083 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%15978, %16081, %16080, %16962), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16084 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%16083, %16083), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16084)
  %v_out.5 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%7, %position_ids.1, %value_states.5), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%16085 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids.13 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states.45 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16963 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16089 : Long(device=cpu) = aten::size(%16085, %16963), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16101 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16102 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16103 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16964 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16105 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16089, %16101, %16102, %16103, %16964), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16704 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16110 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16105, %16704), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16965 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16115 : Long(device=cpu) = aten::size(%16085, %16965), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16124 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16125 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16126 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16966 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16128 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16115, %16124, %16125, %16126, %16966), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16705 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16133 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16128, %16705), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16967 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16135 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.13, %16967), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16706 : int[] = prim::Constant[value=[8, 32, 64]]()
      %16140 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states.45, %16706), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16141 : Tensor?[] = prim::ListConstruct(%16110, %16133, %16135), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16968 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16143 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%16085, %16141, %16140, %16968), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16144 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%16143, %16143), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16144)
  %14173 : Long(device=cpu) = aten::size(%k_out.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %14180 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%14173, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14182 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14180, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14184 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14182, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14187 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %14188 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14190 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14187, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14191 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%14184, %14190), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %ctx_indices.5 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%14191, %16873, %14184), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key.5 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.5, %ctx_indices.5, %14173), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out.45 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.49 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.49 : Long(device=cpu)):
      %16969 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16149 : Long(device=cpu) = aten::size(%k_out.45, %16969), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16161 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16162 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16163 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16970 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16165 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16149, %16161, %16162, %16163, %16970), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16710 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16171 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16165, %16710), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16971 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16176 : Long(device=cpu) = aten::size(%k_out.45, %16971), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16185 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16186 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16187 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16972 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16189 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16176, %16185, %16186, %16187, %16972), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16711 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16195 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16189, %16711), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16973 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16203 : Long(device=cpu) = aten::size(%k_out.45, %16973), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16209 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16210 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16211 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16974 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16213 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16203, %16209, %16210, %16211, %16974), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16712 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16219 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16213, %16712), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16975 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16221 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.49, %16975), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16976 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16223 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16221, %16976, %ctx_indices.49), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16224 : Tensor?[] = prim::ListConstruct(%16171, %16195, %16219, %16223), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16225 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out.45, %16224), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%16225)
  %14273 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.5, %ctx_indices.5, %14173), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out.45 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices.51 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.51 : Long(device=cpu)):
      %16977 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16230 : Long(device=cpu) = aten::size(%v_out.45, %16977), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16242 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16243 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16244 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16978 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16246 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16230, %16242, %16243, %16244, %16978), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16713 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16251 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16246, %16713), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16979 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16256 : Long(device=cpu) = aten::size(%v_out.45, %16979), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16265 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16266 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16267 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16980 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16269 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16256, %16265, %16266, %16267, %16980), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16714 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16274 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16269, %16714), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16981 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16276 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.51, %16981), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16982 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16278 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16276, %16982, %ctx_indices.51), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16279 : Tensor?[] = prim::ListConstruct(%16251, %16274, %16278), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16280 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out.45, %16279), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%16280)
  %14327 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14191, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %value.5 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%14327, %16666, %14273), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %14338 : Long(device=cpu) = aten::size(%key.5, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14343 : Long(device=cpu) = aten::size(%key.5, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14347 : Long(device=cpu) = aten::size(%key.5, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14352 : Long(device=cpu) = aten::size(%key.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14360 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key.5, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14365 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%14360, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14367 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%14365, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14372 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%14367, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14377 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%14372, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14379 : int[] = prim::ListConstruct(%14338, %14343, %16888, %14347, %14352), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14381 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%14377, %14379, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16983 : Long(device=cpu) = prim::Constant[value={4}]()
  %16782 : Long(requires_grad=0, device=cpu) = aten::mul(%14343, %16983), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14385 : int[] = prim::ListConstruct(%14338, %16782, %14347, %14352), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14386 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%14381, %14385), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14388 : Long(device=cpu) = aten::size(%value.5, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14393 : Long(device=cpu) = aten::size(%value.5, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14397 : Long(device=cpu) = aten::size(%value.5, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14402 : Long(device=cpu) = aten::size(%value.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14410 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value.5, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14415 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%14410, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14417 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%14415, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14422 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%14417, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14427 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%14422, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14429 : int[] = prim::ListConstruct(%14388, %14393, %16888, %14397, %14402), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14431 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%14427, %14429, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16984 : Long(device=cpu) = prim::Constant[value={4}]()
  %16784 : Long(requires_grad=0, device=cpu) = aten::mul(%14393, %16984), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14435 : int[] = prim::ListConstruct(%14388, %16784, %14397, %14402), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14436 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%14431, %14435), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14437 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states.11, %14386), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16985 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16786 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%14437, %16985), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %attn_weights.5 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16786), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %14451 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights.5, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %14456 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%14451, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %14457 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%14456, %14436), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %14460 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%14457, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %14462 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%14460, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %14464 : int[] = prim::ListConstruct(%13784, %13791, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14465 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%14462, %14464), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %14467 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%14465, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %14469 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%14467, %model.layers.2.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.11 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.9, %14469, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %14476 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.11, %model.layers.2.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.83 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16282 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16986 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16284 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.83, %16986), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16719 : int[] = prim::Constant[value=[-1]]()
      %16987 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16288 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16289 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16284, %16719, %16987, %16288), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16988 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16989 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16788 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16289, %16989, %16988), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16293 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16788), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16294 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.83, %16293), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16295 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16282, %16294), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16295)
  %input.7 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%14476, %model.layers.2.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14492 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input.7), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %14494 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%14476, %model.layers.2.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14495 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%14492, %14494), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %14497 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%14495, %model.layers.2.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.13 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.11, %14497, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %14504 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.13, %model.layers.3.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.85 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16297 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16990 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16299 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.85, %16990), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16720 : int[] = prim::Constant[value=[-1]]()
      %16991 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16303 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16304 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16299, %16720, %16991, %16303), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16992 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16993 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16790 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16304, %16993, %16992), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16308 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16790), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16309 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.85, %16308), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16310 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16297, %16309), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16310)
  %14519 : Long(device=cpu) = aten::size(%14504, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %14526 : Long(device=cpu) = aten::size(%14504, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %14536 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%14504, %model.layers.3.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14539 : int[] = prim::ListConstruct(%14519, %14526, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14540 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%14536, %14539), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %query_states.13 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%14540, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %14545 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%14504, %model.layers.3.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14549 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%14545, %14539), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.13 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%14549, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %14554 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%14504, %model.layers.3.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14558 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%14554, %14539), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.39 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%14558, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %14576 : Long(device=cpu) = aten::size(%8, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %14602 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %14576, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %14607 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%14602, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16994 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%14607, %16994), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %14614 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %14576, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %14619 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%14614, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16995 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%14619, %16995), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %14625 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14626 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos, %14625), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %14628 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14626, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %14629 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14630 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin, %14629), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %14632 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14630, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %14633 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.13, %14628), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %14644 : Long(device=cpu) = aten::size(%query_states.13, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14647 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%14644, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %14652 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.13, %16839, %16840, %14647, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14671 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.13, %16839, %14647, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %14672 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%14671), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14673 : Tensor[] = prim::ListConstruct(%14672, %14652), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14675 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%14673, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14676 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%14675, %14632), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %14678 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%14633, %14676, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %14679 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.13, %14628), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %14690 : Long(device=cpu) = aten::size(%key_states.13, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14693 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%14690, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %14698 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.13, %16839, %16840, %14693, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14717 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.13, %16839, %14693, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %14718 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%14717), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14719 : Tensor[] = prim::ListConstruct(%14718, %14698), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14721 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%14719, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14722 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%14721, %14632), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %14724 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%14679, %14722, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%14678, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.79 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%14724, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.39 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%8, %position_ids.1, %key_states.79), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%16311 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.15 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16996 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16315 : Long(device=cpu) = aten::size(%16311, %16996), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16327 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16328 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16329 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16997 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16331 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16315, %16327, %16328, %16329, %16997), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16721 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16337 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16331, %16721), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16998 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16342 : Long(device=cpu) = aten::size(%16311, %16998), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16351 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16352 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16353 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16999 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16355 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16342, %16351, %16352, %16353, %16999), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16722 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16361 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16355, %16722), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %17000 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16369 : Long(device=cpu) = aten::size(%16311, %17000), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16375 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16376 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16377 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %17001 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16379 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16369, %16375, %16376, %16377, %17001), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16723 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16385 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16379, %16723), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %17002 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16387 : Long(device=cpu) = aten::size(%16311, %17002), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %17003 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17004 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17005 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16402 : int[] = prim::ListConstruct(%16387, %17003, %17004, %17005), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16403 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.15, %16402), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %17006 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17007 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16406 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states, %17006, %17007), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %17008 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16408 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%16406, %17008), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16724 : int[] = prim::Constant[value=[8, 64, 32]]()
      %16413 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%16408, %16724), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16414 : Tensor?[] = prim::ListConstruct(%16337, %16361, %16385, %16403), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17009 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16416 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%16311, %16414, %16413, %17009), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16417 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%16416, %16416), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16417)
  %v_out.39 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%9, %position_ids.1, %value_states.39), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%16418 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %17010 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16422 : Long(device=cpu) = aten::size(%16418, %17010), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16434 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16435 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16436 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %17011 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16438 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16422, %16434, %16435, %16436, %17011), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16729 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16443 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16438, %16729), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %17012 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16448 : Long(device=cpu) = aten::size(%16418, %17012), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16457 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16458 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16459 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %17013 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16461 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16448, %16457, %16458, %16459, %17013), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16730 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16466 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16461, %16730), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %17014 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16468 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids, %17014), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16731 : int[] = prim::Constant[value=[8, 32, 64]]()
      %16473 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states, %16731), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16474 : Tensor?[] = prim::ListConstruct(%16443, %16466, %16468), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17015 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16476 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%16418, %16474, %16473, %17015), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16477 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%16476, %16476), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16477)
  %14908 : Long(device=cpu) = aten::size(%k_out.39, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %14915 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%14908, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14917 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14915, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14919 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14917, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14922 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %14923 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14925 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14922, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14926 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%14919, %14925), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %ctx_indices.39 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%14926, %16873, %14919), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.39, %ctx_indices.39, %14908), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.53 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.53 : Long(device=cpu)):
      %17016 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16482 : Long(device=cpu) = aten::size(%k_out, %17016), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16494 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16495 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16496 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %17017 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16498 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16482, %16494, %16495, %16496, %17017), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16735 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16504 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16498, %16735), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %17018 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16509 : Long(device=cpu) = aten::size(%k_out, %17018), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16518 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16519 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16520 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %17019 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16522 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16509, %16518, %16519, %16520, %17019), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16736 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16528 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16522, %16736), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %17020 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16536 : Long(device=cpu) = aten::size(%k_out, %17020), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16542 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16543 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16544 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %17021 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16546 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16536, %16542, %16543, %16544, %17021), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16737 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16552 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16546, %16737), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %17022 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16554 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.53, %17022), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %17023 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16556 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16554, %17023, %ctx_indices.53), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16557 : Tensor?[] = prim::ListConstruct(%16504, %16528, %16552, %16556), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16558 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out, %16557), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%16558)
  %15008 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.39, %ctx_indices.39, %14908), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len : Long(device=cpu)):
      %17024 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16563 : Long(device=cpu) = aten::size(%v_out, %17024), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16575 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16576 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16577 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %17025 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16579 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16563, %16575, %16576, %16577, %17025), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16738 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16584 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16579, %16738), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %17026 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16589 : Long(device=cpu) = aten::size(%v_out, %17026), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16598 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16599 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16600 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %17027 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16602 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16589, %16598, %16599, %16600, %17027), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16739 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16607 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16602, %16739), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %17028 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16609 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices, %17028), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %17029 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16611 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16609, %17029, %ctx_indices), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16612 : Tensor?[] = prim::ListConstruct(%16584, %16607, %16611), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16613 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out, %16612), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%16613)
  %15062 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14926, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %value : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%15062, %16666, %15008), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %15073 : Long(device=cpu) = aten::size(%key, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15078 : Long(device=cpu) = aten::size(%key, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15082 : Long(device=cpu) = aten::size(%key, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15087 : Long(device=cpu) = aten::size(%key, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15095 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15100 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%15095, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15102 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%15100, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15107 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%15102, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15112 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%15107, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15114 : int[] = prim::ListConstruct(%15073, %15078, %16888, %15082, %15087), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15116 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%15112, %15114, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %17030 : Long(device=cpu) = prim::Constant[value={4}]()
  %16796 : Long(requires_grad=0, device=cpu) = aten::mul(%15078, %17030), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15120 : int[] = prim::ListConstruct(%15073, %16796, %15082, %15087), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15121 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%15116, %15120), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15123 : Long(device=cpu) = aten::size(%value, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15128 : Long(device=cpu) = aten::size(%value, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15132 : Long(device=cpu) = aten::size(%value, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15137 : Long(device=cpu) = aten::size(%value, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15145 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15150 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%15145, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15152 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%15150, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15157 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%15152, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15162 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%15157, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15164 : int[] = prim::ListConstruct(%15123, %15128, %16888, %15132, %15137), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15166 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%15162, %15164, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %17031 : Long(device=cpu) = prim::Constant[value={4}]()
  %16798 : Long(requires_grad=0, device=cpu) = aten::mul(%15128, %17031), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15170 : int[] = prim::ListConstruct(%15123, %16798, %15132, %15137), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15171 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%15166, %15170), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15172 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states, %15121), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %17032 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16800 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%15172, %17032), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %attn_weights : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16800), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %15186 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %15191 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%15186, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %15192 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%15191, %15171), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %15195 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%15192, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %15197 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%15195, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %15199 : int[] = prim::ListConstruct(%14519, %14526, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15200 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%15197, %15199), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %15202 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%15200, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %15204 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%15202, %model.layers.3.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.15 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.13, %15204, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %15211 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.15, %model.layers.3.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.87 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16615 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %17033 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16617 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.87, %17033), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16744 : int[] = prim::Constant[value=[-1]]()
      %17034 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16621 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16622 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16617, %16744, %17034, %16621), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %17035 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %17036 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16802 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16622, %17036, %17035), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16626 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16802), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16627 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.87, %16626), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16628 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16615, %16627), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16628)
  %input : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%15211, %model.layers.3.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %15227 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %15229 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%15211, %model.layers.3.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %15230 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%15227, %15229), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %15232 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%15230, %model.layers.3.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.71 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.15, %15232, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %15239 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.71, %model.norm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16630 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %17037 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %16632 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states, %17037), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16745 : int[] = prim::Constant[value=[-1]]()
      %17038 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %16636 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %16637 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16632, %16745, %17038, %16636), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %17039 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %17040 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16804 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16637, %17040, %17039), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16641 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.custom[31;20mERROR - QEfficient.base.modeling_qeff - ONNX export or transforms failed: _jit_pass_onnx_set_dynamic_input_shape(): incompatible function arguments. The following argument types are supported:
    1. (arg0: torch::jit::Graph, arg1: dict[str, dict[int, str]], arg2: list[str]) -> None

Invoked with: graph(%input.1 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu),
      %position_ids.1 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu),
      %2 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %3 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %4 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %5 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %6 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %7 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %8 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %9 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %model.embed_tokens.weight : Float(128256, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.0.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.0.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.1.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.1.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.2.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.2.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.3.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.3.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.norm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu)):
  %16837 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/torch.nn.modules.sparse.Embedding::embed_tokens
  %16838 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/torch.nn.modules.sparse.Embedding::embed_tokens
  %hidden_states.1 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::embedding(%model.embed_tokens.weight, %input.1, %16837, %16838, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/torch.nn.modules.sparse.Embedding::embed_tokens # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2551:0
  %16839 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %12262 : Long(device=cpu) = aten::size(%2, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %12284 : Long(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.1, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:43:0
  %16840 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %12286 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %12288 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:44:0
  %12290 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16840, %12262, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:44:0
  %16644 : int[] = prim::Constant[value=[1, 1, -1]]()
  %12295 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::view(%12290, %16644), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:44:0
  %12296 : Bool(1, 32, 32, strides=[1024, 32, 1], requires_grad=0, device=cpu) = aten::gt(%12295, %12284), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:45:0
  %16841 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %attention_mask : Bool(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12296, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:46:0
  %12299 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.1, %model.layers.0.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.73 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15298 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16842 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15300 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.73, %16842), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16645 : int[] = prim::Constant[value=[-1]]()
      %16843 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15304 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15305 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15300, %16645, %16843, %15304), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16844 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16845 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16748 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15305, %16845, %16844), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15309 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16748), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15310 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.73, %15309), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15311 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15298, %15310), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15311)
  %12314 : Long(device=cpu) = aten::size(%12299, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %12321 : Long(device=cpu) = aten::size(%12299, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %12331 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%12299, %model.layers.0.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %16846 : Long(device=cpu) = prim::Constant[value={64}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12334 : int[] = prim::ListConstruct(%12314, %12321, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12335 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%12331, %12334), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %16847 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %query_states.1 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%12335, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %12340 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%12299, %model.layers.0.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %12344 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%12340, %12334), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.1 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%12344, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %12349 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%12299, %model.layers.0.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %12353 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%12349, %12334), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.1 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%12353, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %12371 : Long(device=cpu) = aten::size(%2, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %12393 : Float(131072, 64, strides=[64, 1], requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %12397 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %12371, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16848 : Long(device=cpu) = prim::Constant[value={6}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb
  %12402 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%12397, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16849 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos.1 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%12402, %16849), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %12405 : Float(131072, 64, strides=[64, 1], requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %12409 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %12371, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %12414 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%12409, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16850 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin.1 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%12414, %16850), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %12420 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12421 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos.1, %12420), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %12423 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12421, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %12424 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12425 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin.1, %12424), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %12427 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12425, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %12428 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.1, %12423), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %12439 : Long(device=cpu) = aten::size(%query_states.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %12441 : Long(requires_grad=0, device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %12442 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%12439, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %12447 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.1, %16839, %16840, %12442, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %16851 : Long(device=cpu) = prim::Constant[value={9223372036854775807}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12466 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.1, %16839, %12442, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %12467 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%12466), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12468 : Tensor[] = prim::ListConstruct(%12467, %12447), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12470 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%12468, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12471 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%12470, %12427), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %12473 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%12428, %12471, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %12474 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.1, %12423), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %12485 : Long(device=cpu) = aten::size(%key_states.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %12488 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%12485, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %12493 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.1, %16839, %16840, %12488, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %12512 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.1, %16839, %12488, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %12513 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%12512), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12514 : Tensor[] = prim::ListConstruct(%12513, %12493), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12516 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%12514, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12517 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%12516, %12427), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %12519 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%12474, %12517, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states.3 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%12473, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.3 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%12519, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.1 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%2, %position_ids.1, %key_states.3), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15312 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.3 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states.81 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16852 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15316 : Long(device=cpu) = aten::size(%15312, %16852), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %15328 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15329 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15330 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16853 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15332 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15316, %15328, %15329, %15330, %16853), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16646 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15338 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15332, %16646), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16854 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15343 : Long(device=cpu) = aten::size(%15312, %16854), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %15352 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15353 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15354 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16855 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15356 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15343, %15352, %15353, %15354, %16855), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16647 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15362 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15356, %16647), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16856 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15370 : Long(device=cpu) = aten::size(%15312, %16856), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %15376 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15377 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15378 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16857 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15380 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15370, %15376, %15377, %15378, %16857), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16648 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15386 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15380, %16648), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16858 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15388 : Long(device=cpu) = aten::size(%15312, %16858), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16859 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16860 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16861 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15403 : int[] = prim::ListConstruct(%15388, %16859, %16860, %16861), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15404 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.3, %15403), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16862 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16863 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15407 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states.81, %16862, %16863), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16864 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15409 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%15407, %16864), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16649 : int[] = prim::Constant[value=[8, 64, 32]]()
      %15414 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%15409, %16649), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15415 : Tensor?[] = prim::ListConstruct(%15338, %15362, %15386, %15404), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16865 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15417 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%15312, %15415, %15414, %16865), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15418 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%15417, %15417), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15418)
  %v_out.1 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%3, %position_ids.1, %value_states.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15419 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids.5 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states.41 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16866 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15423 : Long(device=cpu) = aten::size(%15419, %16866), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %15435 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15436 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15437 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16867 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15439 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15423, %15435, %15436, %15437, %16867), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16654 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15444 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15439, %16654), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16868 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15449 : Long(device=cpu) = aten::size(%15419, %16868), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %15458 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15459 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15460 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16869 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15462 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15449, %15458, %15459, %15460, %16869), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16655 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15467 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15462, %16655), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16870 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15469 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.5, %16870), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16656 : int[] = prim::Constant[value=[8, 32, 64]]()
      %15474 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states.41, %16656), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15475 : Tensor?[] = prim::ListConstruct(%15444, %15467, %15469), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16871 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15477 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%15419, %15475, %15474, %16871), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15478 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%15477, %15477), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15478)
  %12703 : Long(device=cpu) = aten::size(%k_out.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %12710 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%12703, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %12712 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12710, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %12714 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12712, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %16872 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12717 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %12718 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %12720 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12717, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %12721 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%12714, %12720), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %16873 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %ctx_indices.1 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%12721, %16873, %12714), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key.1 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.1, %ctx_indices.1, %12703), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out.41 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.41 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.41 : Long(device=cpu)):
      %16874 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15483 : Long(device=cpu) = aten::size(%k_out.41, %16874), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %15495 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15496 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15497 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16875 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15499 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15483, %15495, %15496, %15497, %16875), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16660 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15505 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15499, %16660), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16876 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15510 : Long(device=cpu) = aten::size(%k_out.41, %16876), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %15519 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15520 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15521 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16877 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15523 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15510, %15519, %15520, %15521, %16877), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16661 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15529 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15523, %16661), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16878 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15537 : Long(device=cpu) = aten::size(%k_out.41, %16878), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %15543 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15544 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15545 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16879 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15547 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15537, %15543, %15544, %15545, %16879), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16662 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15553 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15547, %16662), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16880 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15555 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.41, %16880), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16881 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15557 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15555, %16881, %ctx_indices.41), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %15558 : Tensor?[] = prim::ListConstruct(%15505, %15529, %15553, %15557), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15559 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out.41, %15558), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%15559)
  %12803 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.1, %ctx_indices.1, %12703), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out.41 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices.43 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.43 : Long(device=cpu)):
      %16882 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15564 : Long(device=cpu) = aten::size(%v_out.41, %16882), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %15576 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15577 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15578 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16883 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15580 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15564, %15576, %15577, %15578, %16883), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16663 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15585 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15580, %16663), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16884 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15590 : Long(device=cpu) = aten::size(%v_out.41, %16884), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %15599 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15600 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15601 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16885 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15603 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15590, %15599, %15600, %15601, %16885), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16664 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15608 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15603, %16664), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16886 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15610 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.43, %16886), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16887 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15612 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15610, %16887, %ctx_indices.43), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %15613 : Tensor?[] = prim::ListConstruct(%15585, %15608, %15612), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15614 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out.41, %15613), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%15614)
  %12857 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12721, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %16666 : Float(requires_grad=0, device=cpu) = prim::Constant[value={0}]()
  %value.1 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%12857, %16666, %12803), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %12868 : Long(device=cpu) = aten::size(%key.1, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12873 : Long(device=cpu) = aten::size(%key.1, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12877 : Long(device=cpu) = aten::size(%key.1, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12882 : Long(device=cpu) = aten::size(%key.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12890 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key.1, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12895 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%12890, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12897 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%12895, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12902 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%12897, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16888 : Long(device=cpu) = prim::Constant[value={4}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12907 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%12902, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12909 : int[] = prim::ListConstruct(%12868, %12873, %16888, %12877, %12882), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12911 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%12907, %12909, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16889 : Long(device=cpu) = prim::Constant[value={4}]()
  %16754 : Long(requires_grad=0, device=cpu) = aten::mul(%12873, %16889), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12915 : int[] = prim::ListConstruct(%12868, %16754, %12877, %12882), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12916 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%12911, %12915), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12918 : Long(device=cpu) = aten::size(%value.1, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12923 : Long(device=cpu) = aten::size(%value.1, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12927 : Long(device=cpu) = aten::size(%value.1, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12932 : Long(device=cpu) = aten::size(%value.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12940 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value.1, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12945 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%12940, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12947 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%12945, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12952 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%12947, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12957 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%12952, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12959 : int[] = prim::ListConstruct(%12918, %12923, %16888, %12927, %12932), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12961 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%12957, %12959, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16890 : Long(device=cpu) = prim::Constant[value={4}]()
  %16756 : Long(requires_grad=0, device=cpu) = aten::mul(%12923, %16890), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12965 : int[] = prim::ListConstruct(%12918, %16756, %12927, %12932), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12966 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%12961, %12965), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12967 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states.3, %12916), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16891 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16758 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%12967, %16891), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16668 : Float(requires_grad=0, device=cpu) = prim::Constant[value={-inf}]()
  %attn_weights.1 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16758), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %12981 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights.1, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %12986 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%12981, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %12987 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%12986, %12966), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %12990 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%12987, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %12992 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%12990, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %12994 : int[] = prim::ListConstruct(%12314, %12321, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12995 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%12992, %12994), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %12997 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%12995, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %12999 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%12997, %model.layers.0.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.3 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.1, %12999, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %13006 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.3, %model.layers.0.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.75 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15616 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16892 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15618 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.75, %16892), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16669 : int[] = prim::Constant[value=[-1]]()
      %16893 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15622 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15623 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15618, %16669, %16893, %15622), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16894 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16895 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16760 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15623, %16895, %16894), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15627 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16760), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15628 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.75, %15627), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15629 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15616, %15628), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15629)
  %input.3 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13006, %model.layers.0.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13022 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input.3), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %13024 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13006, %model.layers.0.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13025 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%13022, %13024), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %13027 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13025, %model.layers.0.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.5 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.3, %13027, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %13034 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.5, %model.layers.1.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.77 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15631 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16896 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15633 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.77, %16896), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16670 : int[] = prim::Constant[value=[-1]]()
      %16897 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15637 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15638 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15633, %16670, %16897, %15637), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16898 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16899 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16762 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15638, %16899, %16898), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15642 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16762), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15643 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.77, %15642), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15644 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15631, %15643), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15644)
  %13049 : Long(device=cpu) = aten::size(%13034, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13056 : Long(device=cpu) = aten::size(%13034, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13066 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13034, %model.layers.1.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13069 : int[] = prim::ListConstruct(%13049, %13056, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13070 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%13066, %13069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %query_states.5 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%13070, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %13075 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13034, %model.layers.1.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13079 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13075, %13069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.5 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13079, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %13084 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13034, %model.layers.1.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13088 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13084, %13069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.3 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13088, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %13106 : Long(device=cpu) = aten::size(%4, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %13132 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %13106, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13137 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13132, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16900 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos.3 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13137, %16900), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13144 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %13106, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13149 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13144, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16901 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin.3 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13149, %16901), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13155 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13156 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos.3, %13155), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13158 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13156, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13159 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13160 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin.3, %13159), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13162 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13160, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13163 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.5, %13158), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13174 : Long(device=cpu) = aten::size(%query_states.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13177 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13174, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13182 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.5, %16839, %16840, %13177, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13201 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.5, %16839, %13177, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13202 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%13201), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13203 : Tensor[] = prim::ListConstruct(%13202, %13182), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13205 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13203, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13206 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13205, %13162), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13208 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%13163, %13206, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13209 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.5, %13158), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13220 : Long(device=cpu) = aten::size(%key_states.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13223 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13220, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13228 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.5, %16839, %16840, %13223, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13247 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.5, %16839, %13223, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13248 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%13247), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13249 : Tensor[] = prim::ListConstruct(%13248, %13228), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13251 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13249, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13252 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13251, %13162), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13254 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%13209, %13252, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states.7 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%13208, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.7 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%13254, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.3 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%4, %position_ids.1, %key_states.7), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15645 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.7 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states.83 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16902 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15649 : Long(device=cpu) = aten::size(%15645, %16902), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %15661 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15662 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15663 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16903 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15665 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15649, %15661, %15662, %15663, %16903), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16671 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15671 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15665, %16671), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16904 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15676 : Long(device=cpu) = aten::size(%15645, %16904), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %15685 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15686 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15687 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16905 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15689 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15676, %15685, %15686, %15687, %16905), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16672 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15695 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15689, %16672), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16906 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15703 : Long(device=cpu) = aten::size(%15645, %16906), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %15709 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15710 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15711 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16907 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15713 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15703, %15709, %15710, %15711, %16907), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16673 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15719 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15713, %16673), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16908 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15721 : Long(device=cpu) = aten::size(%15645, %16908), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16909 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16910 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16911 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15736 : int[] = prim::ListConstruct(%15721, %16909, %16910, %16911), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15737 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.7, %15736), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16912 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16913 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15740 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states.83, %16912, %16913), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16914 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15742 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%15740, %16914), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16674 : int[] = prim::Constant[value=[8, 64, 32]]()
      %15747 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%15742, %16674), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15748 : Tensor?[] = prim::ListConstruct(%15671, %15695, %15719, %15737), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16915 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15750 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%15645, %15748, %15747, %16915), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15751 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%15750, %15750), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15751)
  %v_out.3 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%5, %position_ids.1, %value_states.3), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15752 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids.9 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states.43 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16916 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15756 : Long(device=cpu) = aten::size(%15752, %16916), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %15768 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15769 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15770 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16917 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15772 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15756, %15768, %15769, %15770, %16917), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16679 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15777 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15772, %16679), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16918 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15782 : Long(device=cpu) = aten::size(%15752, %16918), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %15791 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15792 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15793 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16919 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15795 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15782, %15791, %15792, %15793, %16919), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16680 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15800 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15795, %16680), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16920 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15802 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.9, %16920), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16681 : int[] = prim::Constant[value=[8, 32, 64]]()
      %15807 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states.43, %16681), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15808 : Tensor?[] = prim::ListConstruct(%15777, %15800, %15802), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16921 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15810 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%15752, %15808, %15807, %16921), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15811 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%15810, %15810), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15811)
  %13438 : Long(device=cpu) = aten::size(%k_out.3, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %13445 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%13438, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %13447 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13445, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %13449 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13447, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %13452 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %13453 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %13455 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13452, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %13456 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%13449, %13455), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %ctx_indices.3 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%13456, %16873, %13449), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key.3 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.3, %ctx_indices.3, %13438), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out.43 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.45 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.45 : Long(device=cpu)):
      %16922 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15816 : Long(device=cpu) = aten::size(%k_out.43, %16922), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %15828 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15829 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15830 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16923 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15832 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15816, %15828, %15829, %15830, %16923), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16685 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15838 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15832, %16685), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16924 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15843 : Long(device=cpu) = aten::size(%k_out.43, %16924), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %15852 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15853 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15854 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16925 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15856 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15843, %15852, %15853, %15854, %16925), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16686 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15862 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15856, %16686), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16926 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15870 : Long(device=cpu) = aten::size(%k_out.43, %16926), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %15876 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15877 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15878 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16927 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15880 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15870, %15876, %15877, %15878, %16927), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16687 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15886 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15880, %16687), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16928 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15888 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.45, %16928), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16929 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15890 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15888, %16929, %ctx_indices.45), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %15891 : Tensor?[] = prim::ListConstruct(%15838, %15862, %15886, %15890), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15892 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out.43, %15891), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%15892)
  %13538 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.3, %ctx_indices.3, %13438), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out.43 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices.47 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.47 : Long(device=cpu)):
      %16930 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15897 : Long(device=cpu) = aten::size(%v_out.43, %16930), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %15909 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15910 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15911 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16931 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15913 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15897, %15909, %15910, %15911, %16931), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16688 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15918 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15913, %16688), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16932 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15923 : Long(device=cpu) = aten::size(%v_out.43, %16932), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %15932 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15933 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15934 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16933 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15936 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15923, %15932, %15933, %15934, %16933), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16689 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15941 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15936, %16689), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16934 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15943 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.47, %16934), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16935 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15945 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15943, %16935, %ctx_indices.47), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %15946 : Tensor?[] = prim::ListConstruct(%15918, %15941, %15945), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15947 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out.43, %15946), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%15947)
  %13592 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13456, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %value.3 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%13592, %16666, %13538), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %13603 : Long(device=cpu) = aten::size(%key.3, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13608 : Long(device=cpu) = aten::size(%key.3, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13612 : Long(device=cpu) = aten::size(%key.3, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13617 : Long(device=cpu) = aten::size(%key.3, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13625 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key.3, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13630 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%13625, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13632 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%13630, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13637 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%13632, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13642 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%13637, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13644 : int[] = prim::ListConstruct(%13603, %13608, %16888, %13612, %13617), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13646 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%13642, %13644, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16936 : Long(device=cpu) = prim::Constant[value={4}]()
  %16768 : Long(requires_grad=0, device=cpu) = aten::mul(%13608, %16936), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13650 : int[] = prim::ListConstruct(%13603, %16768, %13612, %13617), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13651 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%13646, %13650), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13653 : Long(device=cpu) = aten::size(%value.3, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13658 : Long(device=cpu) = aten::size(%value.3, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13662 : Long(device=cpu) = aten::size(%value.3, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13667 : Long(device=cpu) = aten::size(%value.3, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13675 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value.3, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13680 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%13675, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13682 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%13680, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13687 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%13682, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13692 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%13687, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13694 : int[] = prim::ListConstruct(%13653, %13658, %16888, %13662, %13667), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13696 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%13692, %13694, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16937 : Long(device=cpu) = prim::Constant[value={4}]()
  %16770 : Long(requires_grad=0, device=cpu) = aten::mul(%13658, %16937), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13700 : int[] = prim::ListConstruct(%13653, %16770, %13662, %13667), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13701 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%13696, %13700), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13702 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states.7, %13651), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16938 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16772 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%13702, %16938), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %attn_weights.3 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16772), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %13716 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights.3, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %13721 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%13716, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %13722 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%13721, %13701), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %13725 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%13722, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %13727 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%13725, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %13729 : int[] = prim::ListConstruct(%13049, %13056, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13730 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%13727, %13729), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %13732 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%13730, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %13734 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13732, %model.layers.1.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.7 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.5, %13734, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %13741 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.7, %model.layers.1.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.79 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15949 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16939 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15951 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.79, %16939), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16694 : int[] = prim::Constant[value=[-1]]()
      %16940 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15955 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15956 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15951, %16694, %16940, %15955), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16941 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16942 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16774 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15956, %16942, %16941), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15960 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16774), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15961 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.79, %15960), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15962 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15949, %15961), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15962)
  %input.5 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13741, %model.layers.1.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13757 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input.5), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %13759 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13741, %model.layers.1.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13760 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%13757, %13759), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %13762 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13760, %model.layers.1.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.9 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.7, %13762, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %13769 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.9, %model.layers.2.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.81 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15964 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16943 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15966 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.81, %16943), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16695 : int[] = prim::Constant[value=[-1]]()
      %16944 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15970 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15971 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15966, %16695, %16944, %15970), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16945 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16946 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16776 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15971, %16946, %16945), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15975 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16776), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15976 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.81, %15975), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15977 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15964, %15976), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15977)
  %13784 : Long(device=cpu) = aten::size(%13769, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13791 : Long(device=cpu) = aten::size(%13769, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13801 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13769, %model.layers.2.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13804 : int[] = prim::ListConstruct(%13784, %13791, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13805 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%13801, %13804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %query_states.9 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%13805, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %13810 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13769, %model.layers.2.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13814 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13810, %13804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.9 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13814, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %13819 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13769, %model.layers.2.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13823 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13819, %13804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.5 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13823, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %13841 : Long(device=cpu) = aten::size(%6, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %13867 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %13841, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13872 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13867, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16947 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos.5 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13872, %16947), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13879 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %13841, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13884 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13879, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16948 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin.5 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13884, %16948), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13890 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13891 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos.5, %13890), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13893 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13891, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13894 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13895 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin.5, %13894), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13897 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13895, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13898 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.9, %13893), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13909 : Long(device=cpu) = aten::size(%query_states.9, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13912 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13909, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13917 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.9, %16839, %16840, %13912, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13936 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.9, %16839, %13912, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13937 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%13936), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13938 : Tensor[] = prim::ListConstruct(%13937, %13917), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13940 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13938, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13941 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13940, %13897), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13943 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%13898, %13941, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13944 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.9, %13893), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13955 : Long(device=cpu) = aten::size(%key_states.9, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13958 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13955, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13963 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.9, %16839, %16840, %13958, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13982 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.9, %16839, %13958, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13983 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%13982), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13984 : Tensor[] = prim::ListConstruct(%13983, %13963), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13986 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13984, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13987 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13986, %13897), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13989 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%13944, %13987, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states.11 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%13943, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.11 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%13989, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.5 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%6, %position_ids.1, %key_states.11), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15978 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.11 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states.85 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16949 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15982 : Long(device=cpu) = aten::size(%15978, %16949), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %15994 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15995 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15996 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16950 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15998 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15982, %15994, %15995, %15996, %16950), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16696 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16004 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15998, %16696), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16951 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16009 : Long(device=cpu) = aten::size(%15978, %16951), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16018 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16019 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16020 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16952 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16022 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16009, %16018, %16019, %16020, %16952), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16697 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16028 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16022, %16697), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16953 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16036 : Long(device=cpu) = aten::size(%15978, %16953), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16042 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16043 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16044 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16954 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16046 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16036, %16042, %16043, %16044, %16954), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16698 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16052 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16046, %16698), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16955 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16054 : Long(device=cpu) = aten::size(%15978, %16955), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16956 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16957 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16958 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16069 : int[] = prim::ListConstruct(%16054, %16956, %16957, %16958), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16070 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.11, %16069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16959 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16960 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16073 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states.85, %16959, %16960), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16961 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16075 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%16073, %16961), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16699 : int[] = prim::Constant[value=[8, 64, 32]]()
      %16080 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%16075, %16699), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16081 : Tensor?[] = prim::ListConstruct(%16004, %16028, %16052, %16070), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16962 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16083 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%15978, %16081, %16080, %16962), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16084 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%16083, %16083), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16084)
  %v_out.5 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%7, %position_ids.1, %value_states.5), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%16085 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids.13 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states.45 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16963 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16089 : Long(device=cpu) = aten::size(%16085, %16963), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16101 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16102 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16103 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16964 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16105 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16089, %16101, %16102, %16103, %16964), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16704 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16110 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16105, %16704), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16965 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16115 : Long(device=cpu) = aten::size(%16085, %16965), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16124 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16125 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16126 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16966 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16128 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16115, %16124, %16125, %16126, %16966), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16705 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16133 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16128, %16705), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16967 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16135 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.13, %16967), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16706 : int[] = prim::Constant[value=[8, 32, 64]]()
      %16140 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states.45, %16706), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16141 : Tensor?[] = prim::ListConstruct(%16110, %16133, %16135), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16968 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16143 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%16085, %16141, %16140, %16968), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16144 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%16143, %16143), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16144)
  %14173 : Long(device=cpu) = aten::size(%k_out.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %14180 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%14173, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14182 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14180, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14184 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14182, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14187 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %14188 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14190 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14187, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14191 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%14184, %14190), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %ctx_indices.5 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%14191, %16873, %14184), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key.5 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.5, %ctx_indices.5, %14173), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out.45 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.49 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.49 : Long(device=cpu)):
      %16969 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16149 : Long(device=cpu) = aten::size(%k_out.45, %16969), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16161 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16162 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16163 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16970 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16165 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16149, %16161, %16162, %16163, %16970), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16710 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16171 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16165, %16710), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16971 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16176 : Long(device=cpu) = aten::size(%k_out.45, %16971), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16185 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16186 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16187 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16972 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16189 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16176, %16185, %16186, %16187, %16972), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16711 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16195 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16189, %16711), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16973 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16203 : Long(device=cpu) = aten::size(%k_out.45, %16973), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16209 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16210 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16211 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16974 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16213 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16203, %16209, %16210, %16211, %16974), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16712 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16219 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16213, %16712), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16975 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16221 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.49, %16975), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16976 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16223 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16221, %16976, %ctx_indices.49), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16224 : Tensor?[] = prim::ListConstruct(%16171, %16195, %16219, %16223), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16225 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out.45, %16224), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%16225)
  %14273 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.5, %ctx_indices.5, %14173), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out.45 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices.51 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.51 : Long(device=cpu)):
      %16977 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16230 : Long(device=cpu) = aten::size(%v_out.45, %16977), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16242 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16243 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16244 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16978 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16246 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16230, %16242, %16243, %16244, %16978), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16713 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16251 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16246, %16713), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16979 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16256 : Long(device=cpu) = aten::size(%v_out.45, %16979), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16265 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16266 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16267 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16980 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16269 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16256, %16265, %16266, %16267, %16980), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16714 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16274 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16269, %16714), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16981 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16276 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.51, %16981), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16982 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16278 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16276, %16982, %ctx_indices.51), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16279 : Tensor?[] = prim::ListConstruct(%16251, %16274, %16278), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16280 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out.45, %16279), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%16280)
  %14327 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14191, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %value.5 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%14327, %16666, %14273), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %14338 : Long(device=cpu) = aten::size(%key.5, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14343 : Long(device=cpu) = aten::size(%key.5, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14347 : Long(device=cpu) = aten::size(%key.5, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14352 : Long(device=cpu) = aten::size(%key.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14360 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key.5, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14365 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%14360, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14367 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%14365, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14372 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%14367, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14377 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%14372, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14379 : int[] = prim::ListConstruct(%14338, %14343, %16888, %14347, %14352), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14381 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%14377, %14379, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16983 : Long(device=cpu) = prim::Constant[value={4}]()
  %16782 : Long(requires_grad=0, device=cpu) = aten::mul(%14343, %16983), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14385 : int[] = prim::ListConstruct(%14338, %16782, %14347, %14352), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14386 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%14381, %14385), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14388 : Long(device=cpu) = aten::size(%value.5, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14393 : Long(device=cpu) = aten::size(%value.5, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14397 : Long(device=cpu) = aten::size(%value.5, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14402 : Long(device=cpu) = aten::size(%value.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14410 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value.5, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14415 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%14410, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14417 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%14415, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14422 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%14417, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14427 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%14422, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14429 : int[] = prim::ListConstruct(%14388, %14393, %16888, %14397, %14402), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14431 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%14427, %14429, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16984 : Long(device=cpu) = prim::Constant[value={4}]()
  %16784 : Long(requires_grad=0, device=cpu) = aten::mul(%14393, %16984), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14435 : int[] = prim::ListConstruct(%14388, %16784, %14397, %14402), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14436 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%14431, %14435), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14437 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states.11, %14386), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16985 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16786 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%14437, %16985), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %attn_weights.5 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16786), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %14451 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights.5, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %14456 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%14451, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %14457 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%14456, %14436), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %14460 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%14457, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %14462 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%14460, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %14464 : int[] = prim::ListConstruct(%13784, %13791, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14465 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%14462, %14464), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %14467 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%14465, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %14469 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%14467, %model.layers.2.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.11 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.9, %14469, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %14476 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.11, %model.layers.2.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.83 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16282 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16986 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16284 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.83, %16986), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16719 : int[] = prim::Constant[value=[-1]]()
      %16987 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16288 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16289 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16284, %16719, %16987, %16288), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16988 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16989 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16788 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16289, %16989, %16988), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16293 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16788), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16294 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.83, %16293), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16295 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16282, %16294), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16295)
  %input.7 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%14476, %model.layers.2.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14492 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input.7), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %14494 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%14476, %model.layers.2.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14495 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%14492, %14494), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %14497 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%14495, %model.layers.2.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.13 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.11, %14497, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %14504 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.13, %model.layers.3.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.85 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16297 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16990 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16299 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.85, %16990), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16720 : int[] = prim::Constant[value=[-1]]()
      %16991 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16303 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16304 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16299, %16720, %16991, %16303), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16992 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16993 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16790 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16304, %16993, %16992), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16308 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16790), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16309 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.85, %16308), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16310 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16297, %16309), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16310)
  %14519 : Long(device=cpu) = aten::size(%14504, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %14526 : Long(device=cpu) = aten::size(%14504, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %14536 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%14504, %model.layers.3.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14539 : int[] = prim::ListConstruct(%14519, %14526, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14540 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%14536, %14539), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %query_states.13 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%14540, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %14545 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%14504, %model.layers.3.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14549 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%14545, %14539), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.13 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%14549, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %14554 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%14504, %model.layers.3.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14558 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%14554, %14539), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.39 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%14558, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %14576 : Long(device=cpu) = aten::size(%8, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %14602 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %14576, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %14607 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%14602, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16994 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%14607, %16994), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %14614 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %14576, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %14619 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%14614, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16995 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%14619, %16995), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %14625 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14626 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos, %14625), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %14628 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14626, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %14629 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14630 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin, %14629), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %14632 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14630, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %14633 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.13, %14628), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %14644 : Long(device=cpu) = aten::size(%query_states.13, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14647 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%14644, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %14652 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.13, %16839, %16840, %14647, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14671 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.13, %16839, %14647, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %14672 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%14671), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14673 : Tensor[] = prim::ListConstruct(%14672, %14652), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14675 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%14673, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14676 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%14675, %14632), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %14678 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%14633, %14676, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %14679 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.13, %14628), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %14690 : Long(device=cpu) = aten::size(%key_states.13, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14693 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%14690, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %14698 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.13, %16839, %16840, %14693, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14717 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.13, %16839, %14693, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %14718 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%14717), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14719 : Tensor[] = prim::ListConstruct(%14718, %14698), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14721 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%14719, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14722 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%14721, %14632), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %14724 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%14679, %14722, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%14678, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.79 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%14724, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.39 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%8, %position_ids.1, %key_states.79), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%16311 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.15 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16996 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16315 : Long(device=cpu) = aten::size(%16311, %16996), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16327 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16328 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16329 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16997 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16331 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16315, %16327, %16328, %16329, %16997), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16721 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16337 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16331, %16721), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16998 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16342 : Long(device=cpu) = aten::size(%16311, %16998), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16351 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16352 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16353 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16999 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16355 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16342, %16351, %16352, %16353, %16999), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16722 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16361 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16355, %16722), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %17000 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16369 : Long(device=cpu) = aten::size(%16311, %17000), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16375 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16376 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16377 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %17001 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16379 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16369, %16375, %16376, %16377, %17001), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16723 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16385 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16379, %16723), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %17002 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16387 : Long(device=cpu) = aten::size(%16311, %17002), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %17003 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17004 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17005 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16402 : int[] = prim::ListConstruct(%16387, %17003, %17004, %17005), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16403 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.15, %16402), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %17006 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17007 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16406 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states, %17006, %17007), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %17008 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16408 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%16406, %17008), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16724 : int[] = prim::Constant[value=[8, 64, 32]]()
      %16413 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%16408, %16724), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16414 : Tensor?[] = prim::ListConstruct(%16337, %16361, %16385, %16403), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17009 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16416 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%16311, %16414, %16413, %17009), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16417 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%16416, %16416), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16417)
  %v_out.39 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%9, %position_ids.1, %value_states.39), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%16418 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %17010 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16422 : Long(device=cpu) = aten::size(%16418, %17010), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16434 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16435 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16436 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %17011 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16438 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16422, %16434, %16435, %16436, %17011), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16729 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16443 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16438, %16729), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %17012 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16448 : Long(device=cpu) = aten::size(%16418, %17012), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16457 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16458 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16459 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %17013 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16461 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16448, %16457, %16458, %16459, %17013), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16730 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16466 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16461, %16730), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %17014 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16468 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids, %17014), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16731 : int[] = prim::Constant[value=[8, 32, 64]]()
      %16473 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states, %16731), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16474 : Tensor?[] = prim::ListConstruct(%16443, %16466, %16468), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17015 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16476 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%16418, %16474, %16473, %17015), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16477 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%16476, %16476), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16477)
  %14908 : Long(device=cpu) = aten::size(%k_out.39, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %14915 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%14908, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14917 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14915, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14919 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14917, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14922 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %14923 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14925 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14922, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14926 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%14919, %14925), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %ctx_indices.39 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%14926, %16873, %14919), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.39, %ctx_indices.39, %14908), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.53 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.53 : Long(device=cpu)):
      %17016 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16482 : Long(device=cpu) = aten::size(%k_out, %17016), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16494 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16495 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16496 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %17017 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16498 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16482, %16494, %16495, %16496, %17017), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16735 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16504 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16498, %16735), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %17018 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16509 : Long(device=cpu) = aten::size(%k_out, %17018), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16518 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16519 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16520 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %17019 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16522 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16509, %16518, %16519, %16520, %17019), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16736 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16528 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16522, %16736), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %17020 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16536 : Long(device=cpu) = aten::size(%k_out, %17020), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16542 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16543 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16544 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %17021 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16546 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16536, %16542, %16543, %16544, %17021), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16737 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16552 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16546, %16737), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %17022 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16554 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.53, %17022), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %17023 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16556 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16554, %17023, %ctx_indices.53), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16557 : Tensor?[] = prim::ListConstruct(%16504, %16528, %16552, %16556), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16558 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out, %16557), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%16558)
  %15008 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.39, %ctx_indices.39, %14908), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len : Long(device=cpu)):
      %17024 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16563 : Long(device=cpu) = aten::size(%v_out, %17024), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16575 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16576 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16577 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %17025 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16579 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16563, %16575, %16576, %16577, %17025), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16738 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16584 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16579, %16738), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %17026 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16589 : Long(device=cpu) = aten::size(%v_out, %17026), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16598 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16599 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16600 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %17027 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16602 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16589, %16598, %16599, %16600, %17027), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16739 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16607 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16602, %16739), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %17028 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16609 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices, %17028), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %17029 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16611 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16609, %17029, %ctx_indices), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16612 : Tensor?[] = prim::ListConstruct(%16584, %16607, %16611), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16613 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out, %16612), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%16613)
  %15062 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14926, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %value : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%15062, %16666, %15008), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %15073 : Long(device=cpu) = aten::size(%key, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15078 : Long(device=cpu) = aten::size(%key, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15082 : Long(device=cpu) = aten::size(%key, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15087 : Long(device=cpu) = aten::size(%key, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15095 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15100 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%15095, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15102 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%15100, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15107 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%15102, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15112 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%15107, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15114 : int[] = prim::ListConstruct(%15073, %15078, %16888, %15082, %15087), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15116 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%15112, %15114, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %17030 : Long(device=cpu) = prim::Constant[value={4}]()
  %16796 : Long(requires_grad=0, device=cpu) = aten::mul(%15078, %17030), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15120 : int[] = prim::ListConstruct(%15073, %16796, %15082, %15087), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15121 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%15116, %15120), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15123 : Long(device=cpu) = aten::size(%value, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15128 : Long(device=cpu) = aten::size(%value, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15132 : Long(device=cpu) = aten::size(%value, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15137 : Long(device=cpu) = aten::size(%value, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15145 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15150 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%15145, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15152 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%15150, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15157 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%15152, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15162 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%15157, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15164 : int[] = prim::ListConstruct(%15123, %15128, %16888, %15132, %15137), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15166 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%15162, %15164, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %17031 : Long(device=cpu) = prim::Constant[value={4}]()
  %16798 : Long(requires_grad=0, device=cpu) = aten::mul(%15128, %17031), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15170 : int[] = prim::ListConstruct(%15123, %16798, %15132, %15137), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15171 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%15166, %15170), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15172 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states, %15121), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %17032 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16800 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%15172, %17032), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %attn_weights : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16800), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %15186 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %15191 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%15186, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %15192 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%15191, %15171), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %15195 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%15192, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %15197 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%15195, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %15199 : int[] = prim::ListConstruct(%14519, %14526, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15200 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%15197, %15199), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %15202 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%15200, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %15204 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%15202, %model.layers.3.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.15 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.13, %15204, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %15211 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.15, %model.layers.3.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.87 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16615 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %17033 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16617 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.87, %17033), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16744 : int[] = prim::Constant[value=[-1]]()
      %17034 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16621 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16622 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16617, %16744, %17034, %16621), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %17035 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %17036 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16802 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16622, %17036, %17035), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16626 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16802), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16627 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.87, %16626), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16628 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16615, %16627), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16628)
  %input : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%15211, %model.layers.3.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %15227 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %15229 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%15211, %model.layers.3.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %15230 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%15227, %15229), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %15232 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%15230, %model.layers.3.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.71 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.15, %15232, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %15239 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.71, %model.norm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16630 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %17037 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %16632 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states, %17037), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16745 : int[] = prim::Constant[value=[-1]]()
      %17038 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %16636 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %16637 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16632, %16745, %17038, %16636), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %17039 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %17040 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16804 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16637, %17040, %17039), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16641 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16642 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states, %16641), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16643 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16630, %16642), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16643)
  %15267 : Int(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::to(%position_ids.1, %16839, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:439:0
  %15270 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::argmax(%15267, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:439:0
  %15272 : Long(device=cpu) = aten::size(%position_ids.1, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %15282 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15272, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %16746 : int[] = prim::Constant[value=[-1, 1]]()
  %15286 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::view(%15282, %16746), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %15287 : Tensor?[] = prim::ListConstruct(%15286, %15270), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::
  %15288 : Float(1, 1, 2048, strides=[2048, 2048, 1], requires_grad=1, device=cpu) = aten::index(%15239, %15287), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %15290 : Float(1, 1, 128256, strides=[128256, 128256, 1], requires_grad=1, device=cpu) = aten::linear(%15288, %model.embed_tokens.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/torch.nn.modules.linear.Linear::lm_head # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %15295 : Float(1, 1, 128256, strides=[128256, 128256, 1], requires_grad=1, device=cpu) = aten::to(%15290, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:441:0
  return (%15295, %k_out.1, %v_out.1, %k_out.3, %v_out.3, %k_out.5, %v_out.5, %k_out.39, %v_out.39)
, {'input_ids': {0: 'batch_size', 1: 'seq_len'}, 'position_ids': {0: 'batch_size', 1: 'seq_len'}, 'past_key.0': 'pkv_dynamic_axes_key[0]', 'past_value.0': 'pkv_dynamic_axes_value[0]', 'past_key.1': 'pkv_dynamic_axes_key[1]', 'past_value.1': 'pkv_dynamic_axes_value[1]', 'past_key.2': 'pkv_dynamic_axes_key[2]', 'past_value.2': 'pkv_dynamic_axes_value[2]', 'past_key.3': 'pkv_dynamic_axes_key[3]', 'past_value.3': 'pkv_dynamic_axes_value[3]'}, ['input_ids', 'position_ids', 'past_key.0', 'past_value.0', 'past_key.1', 'past_value.1', 'past_key.2', 'past_value.2', 'past_key.3', 'past_value.3']  (modeling_qeff.py:315)[0m
op.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16642 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states, %16641), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16643 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16630, %16642), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16643)
  %15267 : Int(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::to(%position_ids.1, %16839, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:439:0
  %15270 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::argmax(%15267, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:439:0
  %15272 : Long(device=cpu) = aten::size(%position_ids.1, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %15282 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15272, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %16746 : int[] = prim::Constant[value=[-1, 1]]()
  %15286 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::view(%15282, %16746), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %15287 : Tensor?[] = prim::ListConstruct(%15286, %15270), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::
  %15288 : Float(1, 1, 2048, strides=[2048, 2048, 1], requires_grad=1, device=cpu) = aten::index(%15239, %15287), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %15290 : Float(1, 1, 128256, strides=[128256, 128256, 1], requires_grad=1, device=cpu) = aten::linear(%15288, %model.embed_tokens.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/torch.nn.modules.linear.Linear::lm_head # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %15295 : Float(1, 1, 128256, strides=[128256, 128256, 1], requires_grad=1, device=cpu) = aten::to(%15290, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:441:0
  return (%15295, %k_out.1, %v_out.1, %k_out.3, %v_out.3, %k_out.5, %v_out.5, %k_out.39, %v_out.39)

Traceback (most recent call last):
  File "/home/abhishek/KV_optimization/org_qeff/quic_abhishek/run.py", line 7, in <module>
    model1.compile(num_devices=1, num_cores=16)
  File "/home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/modeling_auto.py", line 3096, in compile
    qpc_path = self._compile(
  File "/home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/utils/_utils.py", line 704, in wrapper
    result = func(self, *args, **kwargs)
  File "/home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/base/modeling_qeff.py", line 399, in _compile
    else self.get_onnx_path(
  File "/home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/base/modeling_qeff.py", line 348, in get_onnx_path
    self.export(**kwargs)
  File "/home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/modeling_auto.py", line 2733, in export
    return self._export(
  File "/home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/utils/export_utils.py", line 58, in wrapper
    onnx_path = func(self, *args, **kwargs)
  File "/home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/base/modeling_qeff.py", line 316, in _export
    raise e
  File "/home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/base/modeling_qeff.py", line 278, in _export
    torch.onnx.export(
  File "/home/abhishek/eff/lib/python3.10/site-packages/torch/onnx/__init__.py", line 396, in export
    export(
  File "/home/abhishek/eff/lib/python3.10/site-packages/torch/onnx/utils.py", line 529, in export
    _export(
  File "/home/abhishek/eff/lib/python3.10/site-packages/torch/onnx/utils.py", line 1467, in _export
    graph, params_dict, torch_out = _model_to_graph(
  File "/home/abhishek/eff/lib/python3.10/site-packages/torch/onnx/utils.py", line 1091, in _model_to_graph
    graph = _optimize_graph(
  File "/home/abhishek/eff/lib/python3.10/site-packages/torch/onnx/utils.py", line 663, in _optimize_graph
    _C._jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)
TypeError: _jit_pass_onnx_set_dynamic_input_shape(): incompatible function arguments. The following argument types are supported:
    1. (arg0: torch::jit::Graph, arg1: dict[str, dict[int, str]], arg2: list[str]) -> None

Invoked with: graph(%input.1 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu),
      %position_ids.1 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu),
      %2 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %3 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %4 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %5 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %6 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %7 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %8 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu),
      %9 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu),
      %model.embed_tokens.weight : Float(128256, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.0.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.0.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.0.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.1.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.1.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.1.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.2.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.2.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.2.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.q_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.k_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.v_proj.weight : Float(512, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.self_attn.o_proj.weight : Float(2048, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.mlp.gate_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.mlp.up_proj.weight : Float(8192, 2048, strides=[2048, 1], requires_grad=1, device=cpu),
      %model.layers.3.mlp.down_proj.weight : Float(2048, 8192, strides=[8192, 1], requires_grad=1, device=cpu),
      %model.layers.3.input_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.layers.3.post_attention_layernorm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu),
      %model.norm.weight : Float(2048, strides=[1], requires_grad=1, device=cpu)):
  %16837 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/torch.nn.modules.sparse.Embedding::embed_tokens
  %16838 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/torch.nn.modules.sparse.Embedding::embed_tokens
  %hidden_states.1 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::embedding(%model.embed_tokens.weight, %input.1, %16837, %16838, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/torch.nn.modules.sparse.Embedding::embed_tokens # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2551:0
  %16839 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %12262 : Long(device=cpu) = aten::size(%2, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %12284 : Long(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.1, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:43:0
  %16840 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %12286 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %12288 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:44:0
  %12290 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16840, %12262, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:44:0
  %16644 : int[] = prim::Constant[value=[1, 1, -1]]()
  %12295 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::view(%12290, %16644), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:44:0
  %12296 : Bool(1, 32, 32, strides=[1024, 32, 1], requires_grad=0, device=cpu) = aten::gt(%12295, %12284), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:45:0
  %16841 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model
  %attention_mask : Bool(1, 1, 32, 32, strides=[1024, 1024, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12296, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/modeling_attn_mask_utils.py:46:0
  %12299 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.1, %model.layers.0.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.73 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15298 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16842 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15300 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.73, %16842), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16645 : int[] = prim::Constant[value=[-1]]()
      %16843 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15304 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15305 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15300, %16645, %16843, %15304), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16844 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16845 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16748 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15305, %16845, %16844), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15309 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16748), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15310 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.73, %15309), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15311 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15298, %15310), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15311)
  %12314 : Long(device=cpu) = aten::size(%12299, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %12321 : Long(device=cpu) = aten::size(%12299, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %12331 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%12299, %model.layers.0.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %16846 : Long(device=cpu) = prim::Constant[value={64}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12334 : int[] = prim::ListConstruct(%12314, %12321, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12335 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%12331, %12334), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %16847 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %query_states.1 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%12335, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %12340 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%12299, %model.layers.0.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %12344 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%12340, %12334), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.1 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%12344, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %12349 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%12299, %model.layers.0.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %12353 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%12349, %12334), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.1 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%12353, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %12371 : Long(device=cpu) = aten::size(%2, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %12393 : Float(131072, 64, strides=[64, 1], requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %12397 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %12371, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16848 : Long(device=cpu) = prim::Constant[value={6}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb
  %12402 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%12397, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16849 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos.1 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%12402, %16849), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %12405 : Float(131072, 64, strides=[64, 1], requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %12409 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %12371, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %12414 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%12409, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16850 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin.1 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%12414, %16850), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %12420 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12421 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos.1, %12420), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %12423 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12421, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %12424 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12425 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin.1, %12424), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %12427 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12425, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %12428 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.1, %12423), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %12439 : Long(device=cpu) = aten::size(%query_states.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %12441 : Long(requires_grad=0, device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %12442 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%12439, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %12447 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.1, %16839, %16840, %12442, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %16851 : Long(device=cpu) = prim::Constant[value={9223372036854775807}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12466 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.1, %16839, %12442, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %12467 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%12466), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12468 : Tensor[] = prim::ListConstruct(%12467, %12447), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12470 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%12468, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12471 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%12470, %12427), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %12473 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%12428, %12471, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %12474 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.1, %12423), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %12485 : Long(device=cpu) = aten::size(%key_states.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %12488 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%12485, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %12493 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.1, %16839, %16840, %12488, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %12512 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.1, %16839, %12488, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %12513 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%12512), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12514 : Tensor[] = prim::ListConstruct(%12513, %12493), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12516 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%12514, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %12517 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%12516, %12427), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %12519 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%12474, %12517, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states.3 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%12473, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.3 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%12519, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.1 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%2, %position_ids.1, %key_states.3), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15312 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.3 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states.81 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16852 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15316 : Long(device=cpu) = aten::size(%15312, %16852), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %15328 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15329 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15330 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16853 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15332 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15316, %15328, %15329, %15330, %16853), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16646 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15338 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15332, %16646), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16854 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15343 : Long(device=cpu) = aten::size(%15312, %16854), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %15352 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15353 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15354 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16855 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15356 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15343, %15352, %15353, %15354, %16855), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16647 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15362 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15356, %16647), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16856 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15370 : Long(device=cpu) = aten::size(%15312, %16856), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %15376 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15377 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15378 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16857 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15380 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15370, %15376, %15377, %15378, %16857), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16648 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15386 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15380, %16648), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16858 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15388 : Long(device=cpu) = aten::size(%15312, %16858), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16859 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16860 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16861 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15403 : int[] = prim::ListConstruct(%15388, %16859, %16860, %16861), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15404 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.3, %15403), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16862 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16863 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15407 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states.81, %16862, %16863), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16864 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15409 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%15407, %16864), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16649 : int[] = prim::Constant[value=[8, 64, 32]]()
      %15414 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%15409, %16649), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15415 : Tensor?[] = prim::ListConstruct(%15338, %15362, %15386, %15404), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16865 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15417 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%15312, %15415, %15414, %16865), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15418 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%15417, %15417), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15418)
  %v_out.1 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%3, %position_ids.1, %value_states.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15419 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids.5 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states.41 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16866 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15423 : Long(device=cpu) = aten::size(%15419, %16866), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %15435 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15436 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15437 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16867 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15439 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15423, %15435, %15436, %15437, %16867), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16654 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15444 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15439, %16654), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16868 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15449 : Long(device=cpu) = aten::size(%15419, %16868), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %15458 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15459 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15460 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16869 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15462 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15449, %15458, %15459, %15460, %16869), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16655 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15467 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15462, %16655), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16870 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15469 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.5, %16870), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16656 : int[] = prim::Constant[value=[8, 32, 64]]()
      %15474 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states.41, %16656), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15475 : Tensor?[] = prim::ListConstruct(%15444, %15467, %15469), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16871 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15477 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%15419, %15475, %15474, %16871), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15478 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%15477, %15477), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15478)
  %12703 : Long(device=cpu) = aten::size(%k_out.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %12710 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%12703, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %12712 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12710, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %12714 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12712, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %16872 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12717 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %12718 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %12720 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12717, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %12721 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%12714, %12720), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %16873 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %ctx_indices.1 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%12721, %16873, %12714), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key.1 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.1, %ctx_indices.1, %12703), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out.41 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.41 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.41 : Long(device=cpu)):
      %16874 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15483 : Long(device=cpu) = aten::size(%k_out.41, %16874), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %15495 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15496 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15497 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16875 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15499 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15483, %15495, %15496, %15497, %16875), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16660 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15505 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15499, %16660), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16876 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15510 : Long(device=cpu) = aten::size(%k_out.41, %16876), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %15519 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15520 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15521 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16877 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15523 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15510, %15519, %15520, %15521, %16877), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16661 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15529 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15523, %16661), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16878 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15537 : Long(device=cpu) = aten::size(%k_out.41, %16878), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %15543 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15544 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15545 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16879 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15547 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15537, %15543, %15544, %15545, %16879), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16662 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15553 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15547, %16662), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16880 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15555 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.41, %16880), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16881 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15557 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15555, %16881, %ctx_indices.41), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %15558 : Tensor?[] = prim::ListConstruct(%15505, %15529, %15553, %15557), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15559 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out.41, %15558), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%15559)
  %12803 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.1, %ctx_indices.1, %12703), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out.41 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices.43 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.43 : Long(device=cpu)):
      %16882 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15564 : Long(device=cpu) = aten::size(%v_out.41, %16882), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %15576 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15577 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15578 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16883 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15580 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15564, %15576, %15577, %15578, %16883), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16663 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15585 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15580, %16663), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16884 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15590 : Long(device=cpu) = aten::size(%v_out.41, %16884), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %15599 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15600 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15601 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16885 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15603 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15590, %15599, %15600, %15601, %16885), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16664 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15608 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15603, %16664), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16886 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15610 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.43, %16886), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16887 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15612 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15610, %16887, %ctx_indices.43), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %15613 : Tensor?[] = prim::ListConstruct(%15585, %15608, %15612), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15614 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out.41, %15613), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%15614)
  %12857 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%12721, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %16666 : Float(requires_grad=0, device=cpu) = prim::Constant[value={0}]()
  %value.1 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%12857, %16666, %12803), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %12868 : Long(device=cpu) = aten::size(%key.1, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12873 : Long(device=cpu) = aten::size(%key.1, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12877 : Long(device=cpu) = aten::size(%key.1, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12882 : Long(device=cpu) = aten::size(%key.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12890 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key.1, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12895 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%12890, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12897 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%12895, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12902 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%12897, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16888 : Long(device=cpu) = prim::Constant[value={4}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12907 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%12902, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12909 : int[] = prim::ListConstruct(%12868, %12873, %16888, %12877, %12882), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12911 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%12907, %12909, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16889 : Long(device=cpu) = prim::Constant[value={4}]()
  %16754 : Long(requires_grad=0, device=cpu) = aten::mul(%12873, %16889), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12915 : int[] = prim::ListConstruct(%12868, %16754, %12877, %12882), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12916 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%12911, %12915), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12918 : Long(device=cpu) = aten::size(%value.1, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12923 : Long(device=cpu) = aten::size(%value.1, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12927 : Long(device=cpu) = aten::size(%value.1, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12932 : Long(device=cpu) = aten::size(%value.1, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %12940 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value.1, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12945 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%12940, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12947 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%12945, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12952 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%12947, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12957 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%12952, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %12959 : int[] = prim::ListConstruct(%12918, %12923, %16888, %12927, %12932), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12961 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%12957, %12959, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16890 : Long(device=cpu) = prim::Constant[value={4}]()
  %16756 : Long(requires_grad=0, device=cpu) = aten::mul(%12923, %16890), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12965 : int[] = prim::ListConstruct(%12918, %16756, %12927, %12932), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12966 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%12961, %12965), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %12967 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states.3, %12916), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16891 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16758 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%12967, %16891), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16668 : Float(requires_grad=0, device=cpu) = prim::Constant[value={-inf}]()
  %attn_weights.1 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16758), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %12981 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights.1, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %12986 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%12981, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %12987 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%12986, %12966), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %12990 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%12987, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %12992 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%12990, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %12994 : int[] = prim::ListConstruct(%12314, %12321, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %12995 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%12992, %12994), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %12997 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%12995, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %12999 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%12997, %model.layers.0.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.3 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.1, %12999, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %13006 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.3, %model.layers.0.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.75 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15616 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16892 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15618 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.75, %16892), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16669 : int[] = prim::Constant[value=[-1]]()
      %16893 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15622 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15623 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15618, %16669, %16893, %15622), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16894 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16895 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16760 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15623, %16895, %16894), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15627 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16760), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15628 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.75, %15627), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15629 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15616, %15628), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15629)
  %input.3 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13006, %model.layers.0.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13022 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input.3), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %13024 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13006, %model.layers.0.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13025 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%13022, %13024), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %13027 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13025, %model.layers.0.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.5 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.3, %13027, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.0 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %13034 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.5, %model.layers.1.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.77 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15631 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16896 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15633 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.77, %16896), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16670 : int[] = prim::Constant[value=[-1]]()
      %16897 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15637 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15638 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15633, %16670, %16897, %15637), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16898 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16899 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16762 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15638, %16899, %16898), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15642 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16762), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15643 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.77, %15642), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15644 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15631, %15643), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15644)
  %13049 : Long(device=cpu) = aten::size(%13034, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13056 : Long(device=cpu) = aten::size(%13034, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13066 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13034, %model.layers.1.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13069 : int[] = prim::ListConstruct(%13049, %13056, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13070 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%13066, %13069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %query_states.5 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%13070, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %13075 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13034, %model.layers.1.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13079 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13075, %13069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.5 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13079, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %13084 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13034, %model.layers.1.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13088 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13084, %13069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.3 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13088, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %13106 : Long(device=cpu) = aten::size(%4, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %13132 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %13106, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13137 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13132, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16900 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos.3 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13137, %16900), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13144 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %13106, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13149 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13144, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16901 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin.3 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13149, %16901), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13155 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13156 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos.3, %13155), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13158 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13156, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13159 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13160 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin.3, %13159), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13162 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13160, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13163 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.5, %13158), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13174 : Long(device=cpu) = aten::size(%query_states.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13177 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13174, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13182 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.5, %16839, %16840, %13177, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13201 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.5, %16839, %13177, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13202 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%13201), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13203 : Tensor[] = prim::ListConstruct(%13202, %13182), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13205 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13203, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13206 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13205, %13162), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13208 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%13163, %13206, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13209 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.5, %13158), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13220 : Long(device=cpu) = aten::size(%key_states.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13223 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13220, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13228 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.5, %16839, %16840, %13223, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13247 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.5, %16839, %13223, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13248 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%13247), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13249 : Tensor[] = prim::ListConstruct(%13248, %13228), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13251 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13249, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13252 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13251, %13162), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13254 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%13209, %13252, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states.7 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%13208, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.7 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%13254, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.3 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%4, %position_ids.1, %key_states.7), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15645 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.7 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states.83 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16902 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15649 : Long(device=cpu) = aten::size(%15645, %16902), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %15661 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15662 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15663 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16903 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15665 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15649, %15661, %15662, %15663, %16903), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16671 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15671 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15665, %16671), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16904 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15676 : Long(device=cpu) = aten::size(%15645, %16904), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %15685 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15686 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15687 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16905 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15689 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15676, %15685, %15686, %15687, %16905), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16672 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15695 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15689, %16672), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16906 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15703 : Long(device=cpu) = aten::size(%15645, %16906), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %15709 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15710 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15711 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16907 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15713 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15703, %15709, %15710, %15711, %16907), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16673 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15719 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15713, %16673), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16908 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15721 : Long(device=cpu) = aten::size(%15645, %16908), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16909 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16910 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16911 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15736 : int[] = prim::ListConstruct(%15721, %16909, %16910, %16911), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15737 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.7, %15736), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16912 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16913 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15740 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states.83, %16912, %16913), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16914 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15742 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%15740, %16914), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16674 : int[] = prim::Constant[value=[8, 64, 32]]()
      %15747 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%15742, %16674), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15748 : Tensor?[] = prim::ListConstruct(%15671, %15695, %15719, %15737), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16915 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15750 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%15645, %15748, %15747, %16915), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %15751 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%15750, %15750), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15751)
  %v_out.3 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%5, %position_ids.1, %value_states.3), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15752 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids.9 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states.43 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16916 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15756 : Long(device=cpu) = aten::size(%15752, %16916), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %15768 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15769 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15770 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16917 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15772 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15756, %15768, %15769, %15770, %16917), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16679 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15777 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15772, %16679), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16918 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15782 : Long(device=cpu) = aten::size(%15752, %16918), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %15791 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15792 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15793 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16919 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15795 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15782, %15791, %15792, %15793, %16919), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16680 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15800 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15795, %16680), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16920 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15802 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.9, %16920), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16681 : int[] = prim::Constant[value=[8, 32, 64]]()
      %15807 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states.43, %16681), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15808 : Tensor?[] = prim::ListConstruct(%15777, %15800, %15802), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16921 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15810 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%15752, %15808, %15807, %16921), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %15811 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%15810, %15810), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%15811)
  %13438 : Long(device=cpu) = aten::size(%k_out.3, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %13445 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%13438, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %13447 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13445, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %13449 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13447, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %13452 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %13453 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %13455 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13452, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %13456 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%13449, %13455), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %ctx_indices.3 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%13456, %16873, %13449), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key.3 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.3, %ctx_indices.3, %13438), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out.43 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.45 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.45 : Long(device=cpu)):
      %16922 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15816 : Long(device=cpu) = aten::size(%k_out.43, %16922), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %15828 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15829 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15830 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16923 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15832 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15816, %15828, %15829, %15830, %16923), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16685 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %15838 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15832, %16685), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16924 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15843 : Long(device=cpu) = aten::size(%k_out.43, %16924), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %15852 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15853 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15854 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16925 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15856 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15843, %15852, %15853, %15854, %16925), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16686 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %15862 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15856, %16686), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16926 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15870 : Long(device=cpu) = aten::size(%k_out.43, %16926), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %15876 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15877 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15878 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16927 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15880 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15870, %15876, %15877, %15878, %16927), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16687 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %15886 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%15880, %16687), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16928 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15888 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.45, %16928), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16929 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15890 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15888, %16929, %ctx_indices.45), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %15891 : Tensor?[] = prim::ListConstruct(%15838, %15862, %15886, %15890), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15892 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out.43, %15891), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%15892)
  %13538 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.3, %ctx_indices.3, %13438), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out.43 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices.47 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.47 : Long(device=cpu)):
      %16930 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15897 : Long(device=cpu) = aten::size(%v_out.43, %16930), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %15909 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15910 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15911 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16931 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15913 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15897, %15909, %15910, %15911, %16931), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16688 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %15918 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15913, %16688), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16932 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15923 : Long(device=cpu) = aten::size(%v_out.43, %16932), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %15932 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15933 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15934 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16933 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15936 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15923, %15932, %15933, %15934, %16933), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16689 : int[] = prim::Constant[value=[1, -1, 1]]()
      %15941 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%15936, %16689), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16934 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15943 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.47, %16934), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16935 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15945 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%15943, %16935, %ctx_indices.47), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %15946 : Tensor?[] = prim::ListConstruct(%15918, %15941, %15945), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15947 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out.43, %15946), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%15947)
  %13592 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13456, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %value.3 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%13592, %16666, %13538), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %13603 : Long(device=cpu) = aten::size(%key.3, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13608 : Long(device=cpu) = aten::size(%key.3, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13612 : Long(device=cpu) = aten::size(%key.3, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13617 : Long(device=cpu) = aten::size(%key.3, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13625 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key.3, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13630 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%13625, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13632 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%13630, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13637 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%13632, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13642 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%13637, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13644 : int[] = prim::ListConstruct(%13603, %13608, %16888, %13612, %13617), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13646 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%13642, %13644, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16936 : Long(device=cpu) = prim::Constant[value={4}]()
  %16768 : Long(requires_grad=0, device=cpu) = aten::mul(%13608, %16936), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13650 : int[] = prim::ListConstruct(%13603, %16768, %13612, %13617), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13651 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%13646, %13650), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13653 : Long(device=cpu) = aten::size(%value.3, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13658 : Long(device=cpu) = aten::size(%value.3, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13662 : Long(device=cpu) = aten::size(%value.3, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13667 : Long(device=cpu) = aten::size(%value.3, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %13675 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value.3, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13680 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%13675, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13682 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%13680, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13687 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%13682, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13692 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%13687, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %13694 : int[] = prim::ListConstruct(%13653, %13658, %16888, %13662, %13667), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13696 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%13692, %13694, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16937 : Long(device=cpu) = prim::Constant[value={4}]()
  %16770 : Long(requires_grad=0, device=cpu) = aten::mul(%13658, %16937), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13700 : int[] = prim::ListConstruct(%13653, %16770, %13662, %13667), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13701 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%13696, %13700), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %13702 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states.7, %13651), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16938 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16772 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%13702, %16938), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %attn_weights.3 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16772), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %13716 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights.3, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %13721 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%13716, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %13722 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%13721, %13701), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %13725 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%13722, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %13727 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%13725, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %13729 : int[] = prim::ListConstruct(%13049, %13056, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13730 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%13727, %13729), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %13732 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%13730, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %13734 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13732, %model.layers.1.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.7 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.5, %13734, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %13741 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.7, %model.layers.1.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.79 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15949 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16939 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15951 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.79, %16939), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16694 : int[] = prim::Constant[value=[-1]]()
      %16940 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15955 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %15956 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15951, %16694, %16940, %15955), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16941 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16942 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16774 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15956, %16942, %16941), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15960 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16774), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15961 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.79, %15960), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15962 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15949, %15961), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15962)
  %input.5 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13741, %model.layers.1.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13757 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input.5), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %13759 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%13741, %model.layers.1.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13760 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%13757, %13759), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %13762 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13760, %model.layers.1.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.9 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.7, %13762, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.1 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %13769 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.9, %model.layers.2.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.81 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %15964 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16943 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15966 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.81, %16943), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16695 : int[] = prim::Constant[value=[-1]]()
      %16944 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15970 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %15971 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%15966, %16695, %16944, %15970), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16945 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16946 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16776 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%15971, %16946, %16945), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15975 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16776), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15976 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.81, %15975), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %15977 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%15964, %15976), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%15977)
  %13784 : Long(device=cpu) = aten::size(%13769, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13791 : Long(device=cpu) = aten::size(%13769, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %13801 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%13769, %model.layers.2.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13804 : int[] = prim::ListConstruct(%13784, %13791, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13805 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%13801, %13804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %query_states.9 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%13805, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %13810 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13769, %model.layers.2.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13814 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13810, %13804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.9 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13814, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %13819 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%13769, %model.layers.2.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %13823 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%13819, %13804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.5 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%13823, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %13841 : Long(device=cpu) = aten::size(%6, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %13867 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %13841, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13872 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13867, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16947 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos.5 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13872, %16947), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %13879 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %13841, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13884 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%13879, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16948 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin.5 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%13884, %16948), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %13890 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13891 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos.5, %13890), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13893 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13891, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %13894 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13895 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin.5, %13894), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13897 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%13895, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %13898 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.9, %13893), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13909 : Long(device=cpu) = aten::size(%query_states.9, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13912 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13909, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13917 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.9, %16839, %16840, %13912, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13936 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.9, %16839, %13912, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13937 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%13936), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13938 : Tensor[] = prim::ListConstruct(%13937, %13917), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13940 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13938, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13941 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13940, %13897), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13943 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%13898, %13941, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %13944 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.9, %13893), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13955 : Long(device=cpu) = aten::size(%key_states.9, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13958 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%13955, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %13963 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.9, %16839, %16840, %13958, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %13982 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.9, %16839, %13958, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %13983 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%13982), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13984 : Tensor[] = prim::ListConstruct(%13983, %13963), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %13986 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%13984, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %13987 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%13986, %13897), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %13989 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%13944, %13987, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states.11 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%13943, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.11 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%13989, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.5 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%6, %position_ids.1, %key_states.11), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%15978 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.11 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states.85 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16949 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15982 : Long(device=cpu) = aten::size(%15978, %16949), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %15994 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15995 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15996 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16950 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %15998 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15982, %15994, %15995, %15996, %16950), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16696 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16004 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%15998, %16696), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16951 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16009 : Long(device=cpu) = aten::size(%15978, %16951), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16018 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16019 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16020 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16952 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16022 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16009, %16018, %16019, %16020, %16952), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16697 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16028 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16022, %16697), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16953 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16036 : Long(device=cpu) = aten::size(%15978, %16953), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16042 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16043 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16044 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16954 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16046 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16036, %16042, %16043, %16044, %16954), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16698 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16052 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16046, %16698), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16955 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16054 : Long(device=cpu) = aten::size(%15978, %16955), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16956 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16957 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16958 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16069 : int[] = prim::ListConstruct(%16054, %16956, %16957, %16958), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16070 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.11, %16069), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %16959 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16960 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16073 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states.85, %16959, %16960), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16961 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16075 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%16073, %16961), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16699 : int[] = prim::Constant[value=[8, 64, 32]]()
      %16080 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%16075, %16699), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16081 : Tensor?[] = prim::ListConstruct(%16004, %16028, %16052, %16070), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16962 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16083 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%15978, %16081, %16080, %16962), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16084 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%16083, %16083), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16084)
  %v_out.5 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%7, %position_ids.1, %value_states.5), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%16085 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids.13 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states.45 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16963 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16089 : Long(device=cpu) = aten::size(%16085, %16963), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16101 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16102 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16103 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16964 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16105 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16089, %16101, %16102, %16103, %16964), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16704 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16110 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16105, %16704), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16965 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16115 : Long(device=cpu) = aten::size(%16085, %16965), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16124 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16125 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16126 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16966 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16128 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16115, %16124, %16125, %16126, %16966), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16705 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16133 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16128, %16705), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16967 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16135 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids.13, %16967), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16706 : int[] = prim::Constant[value=[8, 32, 64]]()
      %16140 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states.45, %16706), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16141 : Tensor?[] = prim::ListConstruct(%16110, %16133, %16135), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16968 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16143 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%16085, %16141, %16140, %16968), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16144 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%16143, %16143), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16144)
  %14173 : Long(device=cpu) = aten::size(%k_out.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %14180 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%14173, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14182 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14180, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14184 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14182, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14187 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %14188 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14190 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14187, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14191 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%14184, %14190), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %ctx_indices.5 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%14191, %16873, %14184), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key.5 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.5, %ctx_indices.5, %14173), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out.45 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.49 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.49 : Long(device=cpu)):
      %16969 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16149 : Long(device=cpu) = aten::size(%k_out.45, %16969), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16161 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16162 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16163 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16970 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16165 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16149, %16161, %16162, %16163, %16970), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16710 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16171 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16165, %16710), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16971 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16176 : Long(device=cpu) = aten::size(%k_out.45, %16971), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16185 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16186 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16187 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16972 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16189 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16176, %16185, %16186, %16187, %16972), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16711 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16195 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16189, %16711), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16973 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16203 : Long(device=cpu) = aten::size(%k_out.45, %16973), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16209 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16210 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16211 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16974 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16213 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16203, %16209, %16210, %16211, %16974), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16712 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16219 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16213, %16712), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16975 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16221 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.49, %16975), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16976 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16223 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16221, %16976, %ctx_indices.49), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16224 : Tensor?[] = prim::ListConstruct(%16171, %16195, %16219, %16223), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16225 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out.45, %16224), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%16225)
  %14273 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.5, %ctx_indices.5, %14173), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out.45 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices.51 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.51 : Long(device=cpu)):
      %16977 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16230 : Long(device=cpu) = aten::size(%v_out.45, %16977), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16242 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16243 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16244 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16978 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16246 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16230, %16242, %16243, %16244, %16978), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16713 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16251 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16246, %16713), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16979 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16256 : Long(device=cpu) = aten::size(%v_out.45, %16979), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16265 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16266 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16267 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16980 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16269 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16256, %16265, %16266, %16267, %16980), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16714 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16274 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16269, %16714), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16981 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16276 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.51, %16981), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16982 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16278 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16276, %16982, %ctx_indices.51), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16279 : Tensor?[] = prim::ListConstruct(%16251, %16274, %16278), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16280 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out.45, %16279), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%16280)
  %14327 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14191, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %value.5 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%14327, %16666, %14273), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %14338 : Long(device=cpu) = aten::size(%key.5, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14343 : Long(device=cpu) = aten::size(%key.5, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14347 : Long(device=cpu) = aten::size(%key.5, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14352 : Long(device=cpu) = aten::size(%key.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14360 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key.5, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14365 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%14360, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14367 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%14365, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14372 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%14367, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14377 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%14372, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14379 : int[] = prim::ListConstruct(%14338, %14343, %16888, %14347, %14352), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14381 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%14377, %14379, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16983 : Long(device=cpu) = prim::Constant[value={4}]()
  %16782 : Long(requires_grad=0, device=cpu) = aten::mul(%14343, %16983), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14385 : int[] = prim::ListConstruct(%14338, %16782, %14347, %14352), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14386 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%14381, %14385), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14388 : Long(device=cpu) = aten::size(%value.5, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14393 : Long(device=cpu) = aten::size(%value.5, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14397 : Long(device=cpu) = aten::size(%value.5, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14402 : Long(device=cpu) = aten::size(%value.5, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %14410 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value.5, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14415 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%14410, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14417 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%14415, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14422 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%14417, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14427 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%14422, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %14429 : int[] = prim::ListConstruct(%14388, %14393, %16888, %14397, %14402), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14431 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%14427, %14429, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %16984 : Long(device=cpu) = prim::Constant[value={4}]()
  %16784 : Long(requires_grad=0, device=cpu) = aten::mul(%14393, %16984), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14435 : int[] = prim::ListConstruct(%14388, %16784, %14397, %14402), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14436 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%14431, %14435), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %14437 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states.11, %14386), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %16985 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16786 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%14437, %16985), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %attn_weights.5 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16786), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %14451 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights.5, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %14456 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%14451, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %14457 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%14456, %14436), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %14460 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%14457, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %14462 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%14460, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %14464 : int[] = prim::ListConstruct(%13784, %13791, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14465 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%14462, %14464), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %14467 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%14465, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %14469 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%14467, %model.layers.2.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.11 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.9, %14469, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %14476 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.11, %model.layers.2.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.83 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16282 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16986 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16284 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.83, %16986), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16719 : int[] = prim::Constant[value=[-1]]()
      %16987 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16288 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16289 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16284, %16719, %16987, %16288), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16988 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16989 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16788 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16289, %16989, %16988), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16293 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16788), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16294 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.83, %16293), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16295 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16282, %16294), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16295)
  %input.7 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%14476, %model.layers.2.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14492 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input.7), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %14494 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%14476, %model.layers.2.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14495 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%14492, %14494), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %14497 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%14495, %model.layers.2.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.13 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.11, %14497, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.2 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %14504 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.13, %model.layers.3.input_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.85 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16297 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %16990 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16299 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.85, %16990), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16720 : int[] = prim::Constant[value=[-1]]()
      %16991 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16303 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16304 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16299, %16720, %16991, %16303), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16992 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm
      %16993 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16790 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16304, %16993, %16992), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16308 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16790), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16309 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.85, %16308), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16310 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16297, %16309), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::input_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16310)
  %14519 : Long(device=cpu) = aten::size(%14504, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %14526 : Long(device=cpu) = aten::size(%14504, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:217:0
  %14536 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%14504, %model.layers.3.self_attn.q_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::q_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14539 : int[] = prim::ListConstruct(%14519, %14526, %16837, %16846), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14540 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::view(%14536, %14539), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %query_states.13 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%14540, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:225:0
  %14545 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%14504, %model.layers.3.self_attn.k_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::k_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14549 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%14545, %14539), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %key_states.13 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%14549, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:226:0
  %14554 : Float(1, 32, 512, strides=[16384, 512, 1], requires_grad=1, device=cpu) = aten::linear(%14504, %model.layers.3.self_attn.v_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::v_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %14558 : Float(1, 32, 8, 64, strides=[16384, 512, 64, 1], requires_grad=1, device=cpu) = aten::view(%14554, %14539), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %value_states.39 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::transpose(%14558, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:227:0
  %14576 : Long(device=cpu) = aten::size(%8, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:172:0
  %14602 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12393, %16840, %16840, %14576, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %14607 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%14602, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %16994 : Double(device=cpu) = prim::Constant[value={1}]()
  %cos : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%14607, %16994), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:63:0
  %14614 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%12405, %16840, %16840, %14576, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %14619 : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::to(%14614, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %16995 : Double(device=cpu) = prim::Constant[value={1}]()
  %sin : Float(32, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::mul(%14619, %16995), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaRotaryEmbedding::rotary_emb # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:64:0
  %14625 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14626 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%cos, %14625), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %14628 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14626, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:89:0
  %14629 : Tensor?[] = prim::ListConstruct(%position_ids.1), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14630 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%sin, %14629), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %14632 : Float(1, 1, 32, 64, strides=[2048, 2048, 64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14630, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:90:0
  %14633 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::mul(%query_states.13, %14628), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %14644 : Long(device=cpu) = aten::size(%query_states.13, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14647 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%14644, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %14652 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.13, %16839, %16840, %14647, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14671 : Float(1, 32, 32, 32, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::slice(%query_states.13, %16839, %14647, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %14672 : Float(1, 32, 32, 32, strides=[32768, 32, 1024, 1], requires_grad=1, device=cpu) = aten::neg(%14671), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14673 : Tensor[] = prim::ListConstruct(%14672, %14652), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14675 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%14673, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14676 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%14675, %14632), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %14678 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::add(%14633, %14676, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:93:0
  %14679 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::mul(%key_states.13, %14628), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %14690 : Long(device=cpu) = aten::size(%key_states.13, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14693 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%14690, %12441), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/_tensor.py:1137:0
  %14698 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.13, %16839, %16840, %14693, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:108:0
  %14717 : Float(1, 8, 32, 32, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::slice(%key_states.13, %16839, %14693, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109:0
  %14718 : Float(1, 8, 32, 32, strides=[8192, 32, 256, 1], requires_grad=1, device=cpu) = aten::neg(%14717), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14719 : Tensor[] = prim::ListConstruct(%14718, %14698), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %14721 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::cat(%14719, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:110:0
  %14722 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::mul(%14721, %14632), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %14724 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::add(%14679, %14722, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:94:0
  %query_states : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::to(%14678, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %key_states.79 : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu) = aten::to(%14724, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:96:0
  %k_out.39 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%8, %position_ids.1, %key_states.79), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%16311 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu), %position_ids.15 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %key_states : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %16996 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16315 : Long(device=cpu) = aten::size(%16311, %16996), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16327 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16328 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16329 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16997 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16331 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16315, %16327, %16328, %16329, %16997), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16721 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16337 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16331, %16721), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:100:0
      %16998 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16342 : Long(device=cpu) = aten::size(%16311, %16998), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16351 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16352 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16353 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16999 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16355 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16342, %16351, %16352, %16353, %16999), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %16722 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16361 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16355, %16722), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:101:0
      %17000 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16369 : Long(device=cpu) = aten::size(%16311, %17000), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16375 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16376 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16377 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %17001 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16379 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16369, %16375, %16376, %16377, %17001), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %16723 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16385 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16379, %16723), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:102:0
      %17002 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16387 : Long(device=cpu) = aten::size(%16311, %17002), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %17003 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17004 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17005 : Long(device=cpu) = prim::Constant[value={-1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16402 : int[] = prim::ListConstruct(%16387, %17003, %17004, %17005), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16403 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::view(%position_ids.15, %16402), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:103:0
      %17006 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17007 : Long(device=cpu) = prim::Constant[value={3}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16406 : Float(1, 8, 64, 32, strides=[16384, 64, 1, 512], requires_grad=1, device=cpu) = aten::transpose(%key_states, %17006, %17007), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %17008 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16408 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::contiguous(%16406, %17008), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16724 : int[] = prim::Constant[value=[8, 64, 32]]()
      %16413 : Float(8, 64, 32, strides=[2048, 32, 1], requires_grad=0, device=cpu) = aten::view(%16408, %16724), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16414 : Tensor?[] = prim::ListConstruct(%16337, %16361, %16385, %16403), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17009 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16416 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index_put_(%16311, %16414, %16413, %17009), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:104:0
      %16417 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::view_as(%16416, %16416), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16417)
  %v_out.39 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxScatterFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%9, %position_ids.1, %value_states.39), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%16418 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu), %position_ids : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu), %value_states : Float(1, 8, 32, 64, strides=[16384, 64, 512, 1], requires_grad=1, device=cpu)):
      %17010 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16422 : Long(device=cpu) = aten::size(%16418, %17010), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16434 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16435 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16436 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %17011 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16438 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16422, %16434, %16435, %16436, %17011), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %16729 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16443 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16438, %16729), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:107:0
      %17012 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16448 : Long(device=cpu) = aten::size(%16418, %17012), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16457 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16458 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16459 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %17013 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16461 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16448, %16457, %16458, %16459, %17013), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %16730 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16466 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16461, %16730), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:108:0
      %17014 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16468 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%position_ids, %17014), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:109:0
      %16731 : int[] = prim::Constant[value=[8, 32, 64]]()
      %16473 : Float(8, 32, 64, strides=[64, 512, 1], requires_grad=1, device=cpu) = aten::view(%value_states, %16731), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16474 : Tensor?[] = prim::ListConstruct(%16443, %16466, %16468), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %17015 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16476 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index_put_(%16418, %16474, %16473, %17015), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:110:0
      %16477 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::view_as(%16476, %16476), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
      -> (%16477)
  %14908 : Long(device=cpu) = aten::size(%k_out.39, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:218:0
  %14915 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%14908, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14917 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14915, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14919 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14917, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:219:0
  %14922 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu), %14923 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::max(%position_ids.1, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14925 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14922, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:220:0
  %14926 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::gt(%14919, %14925), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:221:0
  %ctx_indices.39 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%14926, %16873, %14919), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:225:0
  %key : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](keys)(%k_out.39, %ctx_indices.39, %14908), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%k_out : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu), %ctx_indices.53 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len.53 : Long(device=cpu)):
      %17016 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16482 : Long(device=cpu) = aten::size(%k_out, %17016), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16494 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16495 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16496 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %17017 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16498 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16482, %16494, %16495, %16496, %17017), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %16735 : int[] = prim::Constant[value=[-1, 1, 1, 1]]()
      %16504 : Long(1, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16498, %16735), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:233:0
      %17018 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16509 : Long(device=cpu) = aten::size(%k_out, %17018), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16518 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16519 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16520 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %17019 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16522 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16509, %16518, %16519, %16520, %17019), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %16736 : int[] = prim::Constant[value=[1, -1, 1, 1]]()
      %16528 : Long(1, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16522, %16736), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:234:0
      %17020 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16536 : Long(device=cpu) = aten::size(%k_out, %17020), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16542 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16543 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16544 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %17021 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16546 : Long(64, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16536, %16542, %16543, %16544, %17021), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %16737 : int[] = prim::Constant[value=[1, 1, -1, 1]]()
      %16552 : Long(1, 1, 64, 1, strides=[64, 64, 1, 1], requires_grad=0, device=cpu) = aten::view(%16546, %16737), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:235:0
      %17022 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16554 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices.53, %17022), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %17023 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16556 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16554, %17023, %ctx_indices.53), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:236:0
      %16557 : Tensor?[] = prim::ListConstruct(%16504, %16528, %16552, %16556), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16558 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=0, device=cpu) = aten::index(%k_out, %16557), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:237:0
      -> (%16558)
  %15008 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = ^CtxGatherFunc[inplace=0, module="QEfficient.customop.ctx_scatter_gather"](values)(%v_out.39, %ctx_indices.39, %14908), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%v_out : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu), %ctx_indices : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu), %ctx_len : Long(device=cpu)):
      %17024 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16563 : Long(device=cpu) = aten::size(%v_out, %17024), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16575 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16576 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16577 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %17025 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16579 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16563, %16575, %16576, %16577, %17025), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %16738 : int[] = prim::Constant[value=[-1, 1, 1]]()
      %16584 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::view(%16579, %16738), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:239:0
      %17026 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16589 : Long(device=cpu) = aten::size(%v_out, %17026), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16598 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16599 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16600 : Device = prim::Constant[value="cpu"](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %17027 : Bool(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16602 : Long(8, strides=[1], requires_grad=0, device=cpu) = aten::arange(%16589, %16598, %16599, %16600, %17027), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %16739 : int[] = prim::Constant[value=[1, -1, 1]]()
      %16607 : Long(1, 8, 1, strides=[8, 1, 1], requires_grad=0, device=cpu) = aten::view(%16602, %16739), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:240:0
      %17028 : Long(device=cpu) = prim::Constant[value={2147483647}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16609 : Bool(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::eq(%ctx_indices, %17028), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %17029 : Long(device=cpu) = prim::Constant[value={0}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16611 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::where(%16609, %17029, %ctx_indices), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:241:0
      %16612 : Tensor?[] = prim::ListConstruct(%16584, %16607, %16611), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
      %16613 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=0, device=cpu) = aten::index(%v_out, %16612), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/ctx_scatter_gather.py:242:0
      -> (%16613)
  %15062 : Bool(1, 1, 32, 1, strides=[32, 32, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%14926, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %value : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::where(%15062, %16666, %15008), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/cache_utils.py:246:0
  %15073 : Long(device=cpu) = aten::size(%key, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15078 : Long(device=cpu) = aten::size(%key, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15082 : Long(device=cpu) = aten::size(%key, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15087 : Long(device=cpu) = aten::size(%key, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15095 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%key, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15100 : Float(1, 8, 64, 32, strides=[16384, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%15095, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15102 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%15100, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15107 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%15102, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15112 : Float(1, 8, 1, 64, 32, strides=[16384, 2048, 2048, 32, 1], requires_grad=1, device=cpu) = aten::slice(%15107, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15114 : int[] = prim::ListConstruct(%15073, %15078, %16888, %15082, %15087), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15116 : Float(1, 8, 4, 64, 32, strides=[16384, 2048, 0, 32, 1], requires_grad=1, device=cpu) = aten::expand(%15112, %15114, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %17030 : Long(device=cpu) = prim::Constant[value={4}]()
  %16796 : Long(requires_grad=0, device=cpu) = aten::mul(%15078, %17030), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15120 : int[] = prim::ListConstruct(%15073, %16796, %15082, %15087), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15121 : Float(1, 32, 64, 32, strides=[65536, 2048, 32, 1], requires_grad=1, device=cpu) = aten::reshape(%15116, %15120), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15123 : Long(device=cpu) = aten::size(%value, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15128 : Long(device=cpu) = aten::size(%value, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15132 : Long(device=cpu) = aten::size(%value, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15137 : Long(device=cpu) = aten::size(%value, %16839), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:161:0
  %15145 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%value, %16840, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15150 : Float(1, 8, 32, 64, strides=[16384, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%15145, %16841, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15152 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%15150, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15157 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%15152, %16839, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15162 : Float(1, 8, 1, 32, 64, strides=[16384, 2048, 2048, 64, 1], requires_grad=1, device=cpu) = aten::slice(%15157, %16888, %16840, %16851, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %15164 : int[] = prim::ListConstruct(%15123, %15128, %16888, %15132, %15137), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15166 : Float(1, 8, 4, 32, 64, strides=[16384, 2048, 0, 64, 1], requires_grad=1, device=cpu) = aten::expand(%15162, %15164, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:164:0
  %17031 : Long(device=cpu) = prim::Constant[value={4}]()
  %16798 : Long(requires_grad=0, device=cpu) = aten::mul(%15128, %17031), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15170 : int[] = prim::ListConstruct(%15123, %16798, %15132, %15137), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15171 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::reshape(%15166, %15170), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:165:0
  %15172 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::matmul(%query_states, %15121), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %17032 : Double(device=cpu) = prim::Constant[value={0.125}]()
  %16800 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::mul(%15172, %17032), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:111:0
  %attn_weights : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::where(%attention_mask, %16668, %16800), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:113:0
  %15186 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::softmax(%attn_weights, %16837, %16848), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2142:0
  %15191 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cpu) = aten::to(%15186, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:117:0
  %15192 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::matmul(%15191, %15171), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:118:0
  %15195 : Float(1, 32, 32, 64, strides=[65536, 64, 2048, 1], requires_grad=1, device=cpu) = aten::transpose(%15192, %16841, %16847), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %15197 : Float(1, 32, 32, 64, strides=[65536, 2048, 64, 1], requires_grad=1, device=cpu) = aten::contiguous(%15195, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:119:0
  %15199 : int[] = prim::ListConstruct(%14519, %14526, %16837), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn
  %15200 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::reshape(%15197, %15199), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %15202 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::contiguous(%15200, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:267:0
  %15204 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%15202, %model.layers.3.self_attn.o_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaAttention::self_attn/torch.nn.modules.linear.Linear::o_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.15 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.13, %15204, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:308:0
  %15211 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.15, %model.layers.3.post_attention_layernorm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states.87 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16615 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %17033 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16617 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.87, %17033), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16744 : int[] = prim::Constant[value=[-1]]()
      %17034 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16621 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %16622 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16617, %16744, %17034, %16621), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %17035 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm
      %17036 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16802 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16622, %17036, %17035), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16626 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16802), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16627 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.87, %16626), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16628 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16615, %16627), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/QEfficient.customop.rms_norm.CustomRMSNormAIC::post_attention_layernorm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16628)
  %input : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%15211, %model.layers.3.mlp.gate_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::gate_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %15227 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::silu(%input), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.activation.SiLU::act_fn # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/functional.py:2380:0
  %15229 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::linear(%15211, %model.layers.3.mlp.up_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::up_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %15230 : Float(1, 32, 8192, strides=[262144, 8192, 1], requires_grad=1, device=cpu) = aten::mul(%15227, %15229), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp # /home/abhishek/eff/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:152:0
  %15232 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::linear(%15230, %model.layers.3.mlp.down_proj.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3/transformers.models.llama.modeling_llama.LlamaMLP::mlp/torch.nn.modules.linear.Linear::down_proj # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %hidden_states.71 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = aten::add(%hidden_states.15, %15232, %16841), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaDecoderLayer::layers.3 # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:314:0
  %15239 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu) = ^CustomRMSNormFunc[inplace=0, module="QEfficient.customop.rms_norm"](1e-05)(%hidden_states.71, %model.norm.weight), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/eff/lib/python3.10/site-packages/torch/autograd/function.py:575:0
    block0(%hidden_states : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=1, device=cpu), %16630 : Float(2048, strides=[1], requires_grad=1, device=cpu)):
      %17037 : Long(device=cpu) = prim::Constant[value={2}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %16632 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states, %17037), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %16745 : int[] = prim::Constant[value=[-1]]()
      %17038 : Bool(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %16636 : NoneType = prim::Constant(), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %16637 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::mean(%16632, %16745, %17038, %16636), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:29:0
      %17039 : Long(device=cpu) = prim::Constant[value={1}](), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm
      %17040 : Double(device=cpu) = prim::Constant[value={1e-05}]()
      %16804 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::add(%16637, %17040, %17039), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16641 : Float(1, 32, 1, strides=[32, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%16804), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16642 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states, %16641), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:30:0
      %16643 : Float(1, 32, 2048, strides=[65536, 2048, 1], requires_grad=0, device=cpu) = aten::mul(%16630, %16642), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/QEfficient.transformers.models.llama.modeling_llama.QEffLlamaModel::model/QEfficient.customop.rms_norm.CustomRMSNormAIC::norm # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/customop/rms_norm.py:31:0
      -> (%16643)
  %15267 : Int(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::to(%position_ids.1, %16839, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:439:0
  %15270 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::argmax(%15267, %16841, %16872), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:439:0
  %15272 : Long(device=cpu) = aten::size(%position_ids.1, %16840), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %15282 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::arange(%15272, %12286, %12286, %12288, %16838), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %16746 : int[] = prim::Constant[value=[-1, 1]]()
  %15286 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::view(%15282, %16746), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %15287 : Tensor?[] = prim::ListConstruct(%15286, %15270), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::
  %15288 : Float(1, 1, 2048, strides=[2048, 2048, 1], requires_grad=1, device=cpu) = aten::index(%15239, %15287), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:440:0
  %15290 : Float(1, 1, 128256, strides=[128256, 128256, 1], requires_grad=1, device=cpu) = aten::linear(%15288, %model.embed_tokens.weight, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM::/torch.nn.modules.linear.Linear::lm_head # /home/abhishek/eff/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0
  %15295 : Float(1, 1, 128256, strides=[128256, 128256, 1], requires_grad=1, device=cpu) = aten::to(%15290, %16848, %16838, %16838, %12286), scope: QEfficient.transformers.models.llama.modeling_llama.QEffLlamaForCausalLM:: # /home/abhishek/KV_optimization/org_qeff/quic_abhishek/QEfficient/transformers/models/llama/modeling_llama.py:441:0
  return (%15295, %k_out.1, %v_out.1, %k_out.3, %v_out.3, %k_out.5, %v_out.5, %k_out.39, %v_out.39)
, {'input_ids': {0: 'batch_size', 1: 'seq_len'}, 'position_ids': {0: 'batch_size', 1: 'seq_len'}, 'past_key.0': 'pkv_dynamic_axes_key[0]', 'past_value.0': 'pkv_dynamic_axes_value[0]', 'past_key.1': 'pkv_dynamic_axes_key[1]', 'past_value.1': 'pkv_dynamic_axes_value[1]', 'past_key.2': 'pkv_dynamic_axes_key[2]', 'past_value.2': 'pkv_dynamic_axes_value[2]', 'past_key.3': 'pkv_dynamic_axes_key[3]', 'past_value.3': 'pkv_dynamic_axes_value[3]'}, ['input_ids', 'position_ids', 'past_key.0', 'past_value.0', 'past_key.1', 'past_value.1', 'past_key.2', 'past_value.2', 'past_key.3', 'past_value.3']
